# OPSEMIA

**Outil Policier de SEMantique et d'Investigation Analytique**

Moteur de recherche sÃ©mantique pour l'analyse de supports numÃ©riques (tÃ©lÃ©phones, ordinateurs) destinÃ© Ã  la police scientifique. Permet la recherche intelligente dans les messages, emails, images et vidÃ©os extraits d'enquÃªtes.

## ğŸ¯ FonctionnalitÃ©s

- **Recherche sÃ©mantique** : Recherche par sens plutÃ´t que par mots-clÃ©s exacts
- **Support multi-formats** : Messages (SMS), emails, images, vidÃ©os
- **Indexation vectorielle** : Utilisation de modÃ¨les d'embedding de pointe (BGE-M3)
- **Chunking contextuel** : FenÃªtre glissante pour prÃ©server le contexte conversationnel
- **DÃ©bruitage** : Filtrage automatique des pubs et contenus commerciaux
- **Base vectorielle locale** : ChromaDB avec backend SQLite pour garantir la confidentialitÃ©
- **Configuration flexible** : Tous les paramÃ¨tres modifiables via `config/settings.py`

## ğŸ“‹ Architecture

```
CSV bruts â†’ Parsing â†’ DÃ©bruitage â†’ Chunking â†’ Encodage (BGE-M3) â†’ ChromaDB (SQLite)
```

- **Parsing modulable** : Support de diffÃ©rentes structures CSV
- **Encodage** : BGE-M3 (1024 dimensions) ou Nomic Embed v2 MoE
- **Chunking** : Messages individuels + fenÃªtres de contexte glissantes
- **Stockage** : Base vectorielle persistante ChromaDB

## ğŸš€ Installation

### 1. PrÃ©requis

- Python 3.9 ou supÃ©rieur
- ~6 GB de RAM minimum
- ~3 GB d'espace disque (pour le modÃ¨le BGE-M3)

### 2. CrÃ©er un environnement virtuel

```bash
python -m venv venv
```

**Activation :**
- Windows : `venv\Scripts\activate`
- Linux/Mac : `source venv/bin/activate`

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 4. TÃ©lÃ©charger le modÃ¨le d'embedding

```bash
python scripts/telecharger_modele_bge.py
```

Cette Ã©tape tÃ©lÃ©charge le modÃ¨le BGE-M3 (~2.2 GB) et le met en cache. Elle peut prendre plusieurs minutes selon votre connexion.

## ğŸ“Š Utilisation

### Indexer un CSV de messages

Le pipeline complet indexe automatiquement le CSV de dÃ©mo (Cas1) :

```bash
python src/backend/core/pipeline_example.py
```

Ce script effectue :
1. **Parsing** du CSV
2. **DÃ©bruitage** (flagging du spam/pubs)
3. **Chunking** (fenÃªtres de contexte glissantes)
4. **Encodage** vectoriel (BGE-M3)
5. **Stockage** dans ChromaDB

### RÃ©sultat

Les donnÃ©es indexÃ©es sont stockÃ©es dans `data/chroma_db/` sous forme de base SQLite locale. Deux collections sont crÃ©Ã©es :
- `messages_cas1` : Messages individuels
- `message_chunks_cas1` : Chunks de contexte

### Logs dÃ©taillÃ©s

Le pipeline affiche des indicateurs de progression et de durÃ©e pour chaque phase :

```
ğŸš€ DÃ©marrage de l'indexation...
ğŸ“„ Phase 1/5: Parsing du CSV...
   âœ“ 250 messages parsÃ©s (0.15s)
ğŸ§¹ Phase 2/5: DÃ©bruitage...
   âœ“ Flags de bruit ajoutÃ©s (0.01s)
ğŸªŸ Phase 3/5: CrÃ©ation des chunks de contexte...
   âœ“ 82 chunks crÃ©Ã©s (fenÃªtre=5, overlap=2) (0.02s)
ğŸ§  Phase 4/5: Encodage vectoriel...
   âœ“ 250 embeddings gÃ©nÃ©rÃ©s (12.34s)
   âœ“ 82 embeddings de chunks gÃ©nÃ©rÃ©s (4.56s)
ğŸ’¾ Phase 5/5: Stockage dans ChromaDB...
   âœ“ Stockage terminÃ© (0.89s)
```

## âš™ï¸ Configuration

Tous les paramÃ¨tres sont configurables dans `config/settings.py` :

### ModÃ¨le d'embedding
```python
ID_MODELE_EMBEDDING = "BAAI/bge-m3"  # ou "nomic-ai/nomic-embed-text-v2-moe"
PERIPHERIQUE_EMBEDDING = "auto"      # "auto" | "cpu" | "cuda"
```

### Chunking
```python
TAILLE_FENETRE_CHUNK = 5   # Nombre de messages par chunk
OVERLAP_FENETRE_CHUNK = 2  # Chevauchement entre chunks
```

### Base vectorielle
```python
CHEMIN_BASE_CHROMA = "data/chroma_db"
NOM_COLLECTION_MESSAGES = "messages"
NOM_COLLECTION_CHUNKS = "message_chunks"
```

## ğŸ“ Structure du projet

```
OPSEMIA/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py              # Configuration centralisÃ©e
â”œâ”€â”€ src/
â”‚   â””â”€â”€ backend/
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â”œâ”€â”€ denoising.py     # DÃ©bruitage (flags spam/pub)
â”‚       â”‚   â”œâ”€â”€ chunking.py      # FenÃªtre glissante
â”‚       â”‚   â””â”€â”€ pipeline_example.py  # Test du pipeline
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ model_manager.py # Gestion singleton encodeur
â”‚       â”‚   â””â”€â”€ text_encoder.py  # Wrapper SentenceTransformer
â”‚       â”œâ”€â”€ parsers/
â”‚       â”‚   â””â”€â”€ message_extractor.py  # Parser CSV messages
â”‚       â””â”€â”€ database/
â”‚           â”œâ”€â”€ vector_db.py     # Wrapper ChromaDB
â”‚           â””â”€â”€ indexer.py       # Pipeline d'indexation
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ telecharger_modele_bge.py  # TÃ©lÃ©chargement modÃ¨le
â”œâ”€â”€ Cas/                         # CSV de dÃ©monstration
â”‚   â”œâ”€â”€ Cas1/
â”‚   â””â”€â”€ Cas2/
â”œâ”€â”€ data/                        # GÃ©nÃ©rÃ© aprÃ¨s indexation
â”‚   â””â”€â”€ chroma_db/              # Base vectorielle SQLite
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”’ ConfidentialitÃ©

**Aucune donnÃ©e n'est transmise Ã  des services tiers.** Tout le traitement est local :
- ModÃ¨les d'embedding tÃ©lÃ©chargÃ©s et exÃ©cutÃ©s localement
- Base vectorielle stockÃ©e localement (SQLite)
- Pas d'API externe ni de tÃ©lÃ©mÃ©trie

## ğŸ› ï¸ DÃ©veloppement en cours

Ce projet est un prototype de hackathon. Phases Ã  venir :
- Moteur de recherche avec filtres (temporel, gÃ©ospatial, type)
- Support images (BLIP + BGE-M3)
- Interface web Flask
- Visualisations (timeline, carte, graphe de relations)
- Fine-tuning pour jargon criminel franÃ§ais

