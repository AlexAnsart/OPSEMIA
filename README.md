# OPSEMIA

**Outil Policier de SEMantique et d'Investigation Analytique**

Moteur de recherche sÃ©mantique pour l'analyse de supports numÃ©riques (tÃ©lÃ©phones, ordinateurs) destinÃ© Ã  la police scientifique. Permet la recherche intelligente dans les messages, emails, images et vidÃ©os extraits d'enquÃªtes.

## ðŸŽ¯ FonctionnalitÃ©s

- **Recherche sÃ©mantique** : Recherche par sens plutÃ´t que par mots-clÃ©s exacts
- **Support multi-formats** : Messages (SMS), emails, images, vidÃ©os
- **Indexation vectorielle** : Utilisation de modÃ¨les d'embedding de pointe (BGE-M3)
- **Chunking contextuel** : FenÃªtre glissante pour prÃ©server le contexte conversationnel
- **DÃ©bruitage** : Filtrage automatique des pubs et contenus commerciaux
- **Base vectorielle locale** : ChromaDB avec backend SQLite pour garantir la confidentialitÃ©
- **Configuration flexible** : Tous les paramÃ¨tres modifiables via `config/settings.py`

## ðŸ“‹ Architecture

```
CSV bruts â†’ Parsing â†’ DÃ©bruitage â†’ Chunking â†’ Encodage (BGE-M3) â†’ ChromaDB (SQLite)
```

- **Parsing modulable** : Support de diffÃ©rentes structures CSV
- **Encodage** : BGE-M3 (1024 dimensions) ou Nomic Embed v2 MoE
- **Chunking** : Messages individuels + fenÃªtres de contexte glissantes
- **Stockage** : Base vectorielle persistante ChromaDB

## ðŸš€ Installation

### 1. PrÃ©requis

- Python 3.9 ou supÃ©rieur
- ~6 GB de RAM minimum
- ~3 GB d'espace disque (pour le modÃ¨le BGE-M3)

### 2. CrÃ©er un environnement virtuel

```bash
python -m venv venv
```

**Activation :**

- Windows : `venv\Scripts\activate`
- Linux/Mac : `source venv/bin/activate`

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 4. TÃ©lÃ©charger le modÃ¨le d'embedding

**Option 1 : BGE-M3 (par dÃ©faut, recommandÃ© pour dÃ©buter)**
```bash
python scripts/telecharger_modele_bge.py
```

**Option 2 : Jina-embeddings-v3 (lÃ©ger et performant)**
```bash
python scripts/telecharger_modele_jina.py
```

**Option 3 : Qwen3-Embedding-8B (haute performance, nÃ©cessite 16+ GB RAM)**
```bash
python scripts/telecharger_modele_qwen3.py
```

Cette Ã©tape tÃ©lÃ©charge le modÃ¨le choisi et le met en cache. Le temps de tÃ©lÃ©chargement varie selon le modÃ¨le (570 MB Ã  8 GB).

### 5. Analyser la base vectorielle (optionnel)

```bash
python scripts/analyser_chromadb.py
```

Ce script analyse le contenu de la base ChromaDB et affiche les statistiques d'indexation.

## ðŸ“Š Utilisation

### Indexer un CSV de messages

Le pipeline complet indexe automatiquement le CSV de dÃ©mo (Cas1) :

```bash
python src/backend/core/pipeline_example.py
```

Ce script effectue :
1. **Parsing** du CSV
2. **DÃ©bruitage** (flagging du spam/pubs)
3. **Chunking** (fenÃªtres de contexte glissantes)
4. **Encodage** vectoriel (BGE-M3)
5. **Stockage** dans ChromaDB

### RÃ©sultat

Les donnÃ©es indexÃ©es sont stockÃ©es dans `data/chroma_db/` sous forme de base vectorielle locale. Deux collections sont crÃ©Ã©es :

- `messages_cas1` : Messages individuels (275 documents)
- `message_chunks_cas1` : Chunks de contexte (1 document)

### Stockage des vecteurs

**Base SQLite** (`chroma.sqlite3`) :

- `collections` : Liste des collections crÃ©Ã©es
- `embeddings` : RÃ©fÃ©rences et IDs des documents
- `embedding_metadata` : MÃ©tadonnÃ©es (timestamp, contact, GPS, direction, etc.)
- `segments` : Liens entre collections et embeddings

**Fichiers binaires** (par segment UUID) :

- `data_level0.bin` : **Vecteurs d'embedding** (float32, 1024 dimensions)
- `header.bin` : MÃ©tadonnÃ©es du fichier
- `length.bin` : Longueurs des vecteurs
- `link_lists.bin` : Structure HNSW pour recherche ANN rapide

**Recherche** : ChromaDB utilise automatiquement HNSW (Hierarchical Navigable Small World) pour la recherche ANN (Approximate Nearest Neighbors) - complexitÃ© O(log n) au lieu de O(n) pour KNN.

### Logs dÃ©taillÃ©s

Le pipeline affiche des indicateurs de progression et de durÃ©e pour chaque phase :

```text
ðŸš€ DÃ©marrage de l'indexation...
ðŸ“„ Phase 1/5: Parsing du CSV...
   âœ“ 250 messages parsÃ©s (0.15s)
ðŸ§¹ Phase 2/5: DÃ©bruitage...
   âœ“ Flags de bruit ajoutÃ©s (0.01s)
ðŸªŸ Phase 3/5: CrÃ©ation des chunks de contexte...
   âœ“ 82 chunks crÃ©Ã©s (fenÃªtre=5, overlap=2) (0.02s)
ðŸ§  Phase 4/5: Encodage vectoriel...
   âœ“ 250 embeddings gÃ©nÃ©rÃ©s (12.34s)
   âœ“ 82 embeddings de chunks gÃ©nÃ©rÃ©s (4.56s)
ðŸ’¾ Phase 5/5: Stockage dans ChromaDB...
   âœ“ Stockage terminÃ© (0.89s)
```

### Recherche sÃ©mantique interactive

**Lancer la recherche :**
```bash
python src/backend/core/pipeline_example.py --search
```

**Exemple de session :**
```text
ðŸ” OPSEMIA - Recherche SÃ©mantique Interactive
======================================================================
ðŸ“š Collection: messages_cas1 (275 documents)
ðŸ§  ModÃ¨le: BAAI/bge-m3
âš™ï¸  MÃ©thode: ANN
ðŸ“Š RÃ©sultats par requÃªte: 10
ðŸš« Exclusion bruit: Oui

ðŸ’¡ Tapez votre requÃªte (ou 'quit' pour quitter)
======================================================================

ðŸ”Ž RequÃªte: rendez-vous argent

â³ Recherche en cours...

âœ… 10 rÃ©sultat(s) trouvÃ©(s):
----------------------------------------------------------------------

1. [Score: 0.842]
   ðŸ“… 2024-03-15 14:23:12 | ðŸ‘¤ Marc Durand | â†”ï¸  incoming
   ðŸ’¬ On se retrouve demain Ã  15h pour le transfert. Prends l'argent liquide...
```

**DÃ©monstration avec filtres :**
```bash
python scripts/demo_recherche_filtree.py
```

**Comparer ANN vs KNN :**
```bash
python scripts/comparer_ann_vs_knn.py
```
Ce script compare la performance et les rÃ©sultats des deux mÃ©thodes sur les mÃªmes requÃªtes.

## âš™ï¸ Configuration

Tous les paramÃ¨tres sont configurables dans `config/settings.py` :

### ModÃ¨le d'embedding
```python
ID_MODELE_EMBEDDING = "BAAI/bge-m3"  # Options disponibles (voir section Benchmark)
PERIPHERIQUE_EMBEDDING = "auto"      # "auto" | "cpu" | "cuda"
```

**ModÃ¨les supportÃ©s :**
- `"BAAI/bge-m3"` - BGE-M3 : Ã‰quilibrÃ©, 1024 dims, ~2.2 GB
- `"jinaai/jina-embeddings-v3"` - Jina-v3 : LÃ©ger et rapide, 1024 dims, ~570 MB
- `"Qwen/Qwen3-Embedding-8B"` - Qwen3-8B : Haute performance, 4096 dims, ~8 GB

### Chunking
```python
TAILLE_FENETRE_CHUNK = 5   # Nombre de messages par chunk
OVERLAP_FENETRE_CHUNK = 2  # Chevauchement entre chunks
```

### Base vectorielle
```python
CHEMIN_BASE_CHROMA = "data/chroma_db"
NOM_COLLECTION_MESSAGES = "messages"
NOM_COLLECTION_CHUNKS = "message_chunks"
```

### Recherche
```python
METHODE_RECHERCHE = "ANN"              # "ANN" (rapide) | "KNN" (exact)
NOMBRE_RESULTATS_RECHERCHE = 10        # Nombre de rÃ©sultats (top K)
EXCLURE_BRUIT_PAR_DEFAUT = True        # Filtrer automatiquement le spam/pub
SEUIL_DISTANCE_MAX = None              # Distance cosine max (None = pas de seuil)
```

**ANN vs KNN** :
- **ANN** (Approximate Nearest Neighbors) : Utilise l'index HNSW pour une recherche ultra-rapide. ComplexitÃ© O(log n). PrÃ©cision ~95-99%. **RecommandÃ© pour la production.**
- **KNN** (K-Nearest Neighbors) : Recherche exhaustive exacte avec calcul manuel des distances cosine. ComplexitÃ© O(n). 100% prÃ©cis, idÃ©al pour validation ou petites bases.

**Changement de mÃ©thode** : Modifier `METHODE_RECHERCHE` dans `config/settings.py` puis relancer la recherche. Aucune rÃ©indexation nÃ©cessaire.

## ðŸ“Š Benchmark des modÃ¨les d'embedding

OPSEMIA intÃ¨gre un systÃ¨me de benchmark complet pour Ã©valuer et comparer diffÃ©rents modÃ¨les d'embedding.

### Lancer le benchmark

```bash
python scripts/benchmark_modeles.py
```

Ce script Ã©value automatiquement plusieurs modÃ¨les sur un dataset de test thÃ©matique (enquÃªtes policiÃ¨res) et calcule :
- **NDCG@5** : QualitÃ© du classement (mÃ©trique principale)
- **MRR** : Position du premier rÃ©sultat pertinent
- **Precision@K / Recall@K** : Pertinence des rÃ©sultats
- **Temps de chargement et recherche**


## ðŸ“ Structure du projet

```
OPSEMIA/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py              # Configuration centralisÃ©e
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ api/                 # API REST Flask
â”‚   â”‚   â”‚   â”œâ”€â”€ routes_indexation.py   # Routes chargement CSV
â”‚   â”‚   â”‚   â”œâ”€â”€ routes_recherche.py    # Routes recherche sÃ©mantique
â”‚   â”‚   â”‚   â”œâ”€â”€ routes_conversations.py # Routes conversations
â”‚   â”‚   â”‚   â”œâ”€â”€ routes_donnees.py      # Routes accÃ¨s donnÃ©es
â”‚   â”‚   â”‚   â””â”€â”€ routes_config.py       # Routes configuration
â”‚   â”‚   â”œâ”€â”€ app.py               # Serveur Flask principal
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ denoising.py     # DÃ©bruitage (flags spam/pub)
â”‚   â”‚   â”‚   â”œâ”€â”€ chunking.py      # FenÃªtre glissante
â”‚   â”‚   â”‚   â”œâ”€â”€ filters.py       # Filtres de recherche (temps, GPS, type)
â”‚   â”‚   â”‚   â”œâ”€â”€ search_engine.py # Moteur de recherche sÃ©mantique
â”‚   â”‚   â”‚   â””â”€â”€ pipeline_example.py  # Pipeline + recherche interactive
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ model_manager.py # Gestion singleton encodeur
â”‚   â”‚   â”‚   â””â”€â”€ text_encoder.py  # Wrapper SentenceTransformer
â”‚   â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”‚   â””â”€â”€ message_extractor.py  # Parser CSV messages
â”‚   â”‚   â””â”€â”€ database/
â”‚   â”‚       â”œâ”€â”€ vector_db.py     # Wrapper ChromaDB + recherche
â”‚   â”‚       â””â”€â”€ indexer.py       # Pipeline d'indexation
â”‚   â””â”€â”€ frontend/                # Interface web
â”‚       â”œâ”€â”€ templates/
â”‚       â”‚   â”œâ”€â”€ index.html       # Page de recherche
â”‚       â”‚   â”œâ”€â”€ conversations.html # Page de conversations
â”‚       â”‚   â””â”€â”€ gestion.html     # Page de gestion
â”‚       â””â”€â”€ static/
â”‚           â”œâ”€â”€ css/
â”‚           â”‚   â””â”€â”€ main.css     # Styles principaux
â”‚           â””â”€â”€ js/
â”‚               â”œâ”€â”€ api.js       # Communication avec backend
â”‚               â”œâ”€â”€ search.js    # Logique de recherche
â”‚               â”œâ”€â”€ conversations.js # Gestion des conversations
â”‚               â”œâ”€â”€ context.js   # Vue contextuelle
â”‚               â””â”€â”€ gestion.js   # Gestion et configuration
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ telecharger_modele_bge.py  # TÃ©lÃ©chargement BGE-M3
â”‚   â”œâ”€â”€ telecharger_modele_jina.py # TÃ©lÃ©chargement Jina-v3
â”‚   â”œâ”€â”€ telecharger_modele_qwen3.py # TÃ©lÃ©chargement Qwen3-8B
â”‚   â”œâ”€â”€ telecharger_modele_solon.py # TÃ©lÃ©chargement Solon (Ã  confirmer)
â”‚   â”œâ”€â”€ benchmark_modeles.py       # Benchmark comparatif des modÃ¨les
â”‚   â”œâ”€â”€ donnees_benchmark.py       # Dataset de test pour benchmark
â”‚   â”œâ”€â”€ analyser_chromadb.py       # Analyse de la base vectorielle
â”‚   â”œâ”€â”€ demo_recherche_filtree.py  # DÃ©monstration filtres
â”‚   â”œâ”€â”€ comparer_ann_vs_knn.py     # Comparaison ANN vs KNN
â”‚   â””â”€â”€ tester_api.py              # Tests de l'API REST
â”œâ”€â”€ Docs Projet/
â”‚   â”œâ”€â”€ API_Documentation.md          # Documentation complÃ¨te de l'API
â”‚   â”œâ”€â”€ Guide_Modeles_Benchmark.md    # Guide modÃ¨les et benchmark
â”‚   â””â”€â”€ Guide_Phase8_Frontend.md      # Guide dÃ©veloppement frontend (si existant)
â”œâ”€â”€ Cas/                         # CSV de dÃ©monstration
â”‚   â”œâ”€â”€ Cas1/
â”‚   â””â”€â”€ Cas2/
â”œâ”€â”€ data/                        # GÃ©nÃ©rÃ© aprÃ¨s indexation
â”‚   â””â”€â”€ chroma_db/              # Base vectorielle SQLite
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ðŸ”’ ConfidentialitÃ©

**Aucune donnÃ©e n'est transmise Ã  des services tiers.** Tout le traitement est local :

- ModÃ¨les d'embedding tÃ©lÃ©chargÃ©s et exÃ©cutÃ©s localement
- Base vectorielle stockÃ©e localement (SQLite)
- Pas d'API externe ni de tÃ©lÃ©mÃ©trie

## ðŸ” Analyse de la base vectorielle

Le script `scripts/analyser_chromadb.py` permet d'inspecter le contenu de la base :

```bash
python scripts/analyser_chromadb.py
```

**Sortie typique** :

```text
=== ANALYSE CHROMADB ===
Collections trouvÃ©es: 2
- messages_cas1: 275 documents
- message_chunks_cas1: 1 document

=== METADONNEES (exemple) ===
ID: msg_0
Timestamp: 2024-01-15 14:30:22
Contact: +33123456789
Direction: incoming
GPS: 48.8566, 2.3522
```

## ðŸ” Utilisation avancÃ©e

### Recherche programmatique

```python
from config.settings import obtenir_parametres
from src.backend.core.search_engine import MoteurRecherche
from src.backend.core.filters import creer_filtre_temporel
from src.backend.database.vector_db import BaseVectorielle

# Initialisation
parametres = obtenir_parametres()
db = BaseVectorielle(parametres.CHEMIN_BASE_CHROMA)
moteur = MoteurRecherche(db, parametres)

# Recherche simple
resultats = moteur.rechercher(
    requete="rendez-vous suspect",
    nom_collection="messages_cas1",
    nombre_resultats=5
)

# Recherche avec filtres
filtre = creer_filtre_temporel("2024-03-01", "2024-03-31")
resultats = moteur.rechercher(
    requete="transfert argent",
    nom_collection="messages_cas1",
    filtres=filtre,
    exclure_bruit=True
)

# Afficher
for res in resultats:
    print(f"[{res['score']:.3f}] {res['document'][:100]}")
```

### Filtres disponibles

- **Temporel** : `creer_filtre_temporel("2024-01-01", "2024-12-31")`
- **GÃ©ographique** : `creer_filtre_geographique(lat, lon, rayon_km)`
- **Exclusion bruit** : `creer_filtre_exclusion_bruit(exclure=True)`
- **Combinaison** : `combiner_filtres(filtre1, filtre2)`

### Scores de pertinence

- **> 0.8** : TrÃ¨s pertinent
- **0.6 - 0.8** : Pertinent  
- **< 0.6** : Peu pertinent

## ðŸ–¥ï¸ Interface Web

OPSEMIA dispose d'une interface web moderne pour faciliter l'utilisation par les analystes de la police scientifique.

### DÃ©marrage

```bash
python src/backend/app.py
```

Ouvrez votre navigateur Ã  `http://127.0.0.1:5000`

### Pages disponibles

#### ðŸ” Page de Recherche (`/`)
- Barre de recherche sÃ©mantique en langage naturel
- SÃ©lection de la collection Ã  interroger
- Filtres avancÃ©s (dates, direction, exclusion du bruit)
- Affichage des rÃ©sultats avec score de pertinence
- **Vue contextuelle** : Cliquez sur un rÃ©sultat pour voir la conversation complÃ¨te avec messages avant/aprÃ¨s
- **Navigation vers conversations** : Bouton pour voir le message dans son contexte conversationnel complet

#### ðŸ’¬ Page de Conversations (`/conversations`)
- **Vue complÃ¨te des conversations** : Interface type messagerie avec liste des conversations et messages
- **Filtrage et recherche** : Recherche par nom, numÃ©ro ou contenu dans les conversations
- **Bulles de messages** : Affichage chronologique avec distinction messages reÃ§us/envoyÃ©s
- **MÃ©tadonnÃ©es** : GPS, application, flags spam accessibles via badges discrets
- **Navigation contextuelle** : IntÃ©gration avec la recherche sÃ©mantique - cliquez sur "Voir dans les conversations" depuis un rÃ©sultat de recherche pour accÃ©der directement au message dans sa conversation complÃ¨te
- **Recherche intra-conversation** : Recherche par mots-clÃ©s dans une conversation spÃ©cifique

#### âš™ï¸ Page de Gestion (`/gestion`)
- **Charger des CSV** : Indexer un fichier ou un dossier complet
- **Statistiques** : Nombre de documents, collections, modÃ¨le utilisÃ©
- **Configuration** : Changer mÃ©thode de recherche (ANN/KNN), nombre de rÃ©sultats, filtres par dÃ©faut
- **Guide rapide** : Instructions pour bien utiliser l'outil

### FonctionnalitÃ©s clÃ©s

**Recherche sÃ©mantique** : Utilisez des requÃªtes en langage naturel (ex: "rendez-vous argent suspect", "transfert drogue")

**Vue contextuelle** : En cliquant sur un rÃ©sultat, un panneau latÃ©ral s'ouvre affichant :
- Le message recherchÃ© (surlignÃ©)
- 10 messages avant (contexte prÃ©cÃ©dent)
- 10 messages aprÃ¨s (contexte suivant)
- MÃ©tadonnÃ©es complÃ¨tes (date, contact, direction, GPS)

**Filtres avancÃ©s** :
- Plage temporelle (dates de dÃ©but et fin)
- Direction (reÃ§us/envoyÃ©s)
- Exclusion automatique du spam/pub

**Design professionnel** : Interface moderne adaptÃ©e au contexte police scientifique, responsive, avec thÃ¨me bleu foncÃ©.

## ðŸŒ API REST

OPSEMIA expose une API REST complÃ¨te pour intÃ©gration avec un frontend web ou autres applications.

### DÃ©marrage du serveur

```bash
python src/backend/app.py
```

Le serveur dÃ©marre sur `http://127.0.0.1:5000`.

### Endpoints principaux

#### Recherche
- `POST /api/search` - Recherche sÃ©mantique avec filtres
- `GET /api/message/<id>` - Obtenir un message spÃ©cifique
- `GET /api/context/<id>` - Obtenir le contexte d'un message

#### Conversations
- `GET /api/conversations` - Lister toutes les conversations (groupÃ©es par contact)
- `GET /api/conversation/<contact>` - Obtenir tous les messages d'une conversation
- `POST /api/conversation/<contact>/search` - Rechercher dans une conversation spÃ©cifique

#### Indexation
- `POST /api/load` - Charger et indexer un fichier CSV
- `POST /api/load_dossier` - Charger tous les CSV d'un dossier

#### Configuration
- `GET /api/config` - Obtenir la configuration actuelle
- `POST /api/config` - Modifier la configuration
- `GET /api/stats` - Statistiques d'indexation
- `GET /api/collections` - Lister les collections
- `GET /api/health` - VÃ©rifier la santÃ© de l'API

### Exemple de requÃªte

```bash
curl -X POST http://127.0.0.1:5000/api/search \
  -H "Content-Type: application/json" \
  -d '{
    "requete": "rendez-vous argent",
    "nom_collection": "messages_cas1",
    "nombre_resultats": 5,
    "exclure_bruit": true,
    "filtres": {
      "timestamp_debut": "2024-01-01",
      "timestamp_fin": "2024-12-31"
    }
  }'
```

### Tests de l'API

```bash
# Terminal 1 : DÃ©marrer le serveur
python src/backend/app.py

# Terminal 2 : Lancer les tests
python scripts/tester_api.py
```

**Documentation complÃ¨te** : Voir `Docs Projet/API_Documentation.md`


## ðŸ“Š Architecture technique

**Pipeline complet :**
```
CSV â†’ Parser â†’ DÃ©bruitage â†’ Chunking â†’ BGE-M3 â†’ ChromaDB (HNSW)
```

**Performance (sur 275 messages) :**
- **Indexation** : ~15-20s
- **Recherche ANN (HNSW)** : < 0.1s (95-99% prÃ©cision)
- **Recherche KNN (exact)** : < 0.5s (100% prÃ©cision)

**ImplÃ©mentation KNN** : RÃ©cupÃ¨re tous les embeddings, calcule manuellement les distances cosine avec NumPy, trie et retourne les top K. Garantit une prÃ©cision exacte contrairement Ã  l'approximation HNSW.

**Fichiers clÃ©s :**
- `src/backend/core/search_engine.py` : Moteur de recherche
- `src/backend/core/filters.py` : Filtres temporels/GPS/bruit
- `src/backend/database/vector_db.py` : Interface ChromaDB
- `src/backend/app.py` : Serveur API Flask
- `config/settings.py` : Configuration (ANN/KNN, K, filtres)