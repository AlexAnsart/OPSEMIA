"""Encodeur d'images utilisant BLIP pour la description et BGE pour l'embedding.

Ce module g√©n√®re des descriptions textuelles d'images en fran√ßais, puis les encode
en vecteurs pour la recherche s√©mantique.
"""

from __future__ import annotations

import io
import sys
from pathlib import Path
from typing import Iterable, List, Optional, Union

import numpy as np
import torch
from PIL import Image
from transformers import (
    BlipForConditionalGeneration,
    BlipProcessor,
    MarianMTModel,
    MarianTokenizer,
)

# Forcer UTF-8 pour la sortie console
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
if sys.stderr.encoding != 'utf-8':
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


class EncodeurImage:
    """Encodeur d'images en deux √©tapes:
    1. G√©n√©ration de description textuelle en fran√ßais (BLIP + traduction)
    2. Encodage de la description en vecteur (m√™me mod√®le que pour les messages)
    
    Attributs:
        peripherique: Device PyTorch (cpu/cuda)
        processor_blip: Processeur BLIP pour les images
        modele_blip: Mod√®le BLIP pour le captioning
        tokenizer_trad: Tokenizer pour la traduction EN->FR
        modele_trad: Mod√®le de traduction EN->FR
        encodeur_texte: Encodeur de texte (partag√© avec les messages)
    """
    
    def __init__(
        self,
        encodeur_texte,
        preference_peripherique: str = "auto",
        longueur_min_description: int = 30,
        longueur_max_description: int = 150,
        num_beams: int = 15,
        temperature: float = 0.3,
    ) -> None:
        """Initialise l'encodeur d'images.
        
        Args:
            encodeur_texte: Instance de EncodeurTexte pour encoder les descriptions
            preference_peripherique: "auto", "cpu" ou "cuda"
            longueur_min_description: Longueur minimale de la description en tokens
            longueur_max_description: Longueur maximale de la description en tokens
            num_beams: Nombre de beams pour la g√©n√©ration (qualit√©)
            temperature: Temp√©rature de sampling (cr√©ativit√©)
        """
        self.peripherique: str = self._resoudre_peripherique(preference_peripherique)
        self.encodeur_texte = encodeur_texte
        
        # Param√®tres de g√©n√©ration
        self.longueur_min = longueur_min_description
        self.longueur_max = longueur_max_description
        self.num_beams = num_beams
        self.temperature = temperature
        
        # Charger les mod√®les
        print(f"üñºÔ∏è  Chargement du mod√®le BLIP pour description d'images...")
        self.processor_blip = BlipProcessor.from_pretrained(
            "Salesforce/blip-image-captioning-base"
        )
        self.modele_blip = BlipForConditionalGeneration.from_pretrained(
            "Salesforce/blip-image-captioning-base"
        ).to(self.peripherique)
        
        print(f"üåç Chargement du mod√®le de traduction EN->FR...")
        self.tokenizer_trad = MarianTokenizer.from_pretrained(
            "Helsinki-NLP/opus-mt-en-fr"
        )
        self.modele_trad = MarianMTModel.from_pretrained(
            "Helsinki-NLP/opus-mt-en-fr"
        ).to(self.peripherique)
        
        print(f"‚úÖ Encodeur d'images pr√™t (p√©riph√©rique: {self.peripherique})")
    
    def _resoudre_peripherique(self, preference: str) -> str:
        """R√©sout le p√©riph√©rique √† utiliser."""
        if preference == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        if preference in {"cpu", "cuda"}:
            return preference
        return "cpu"
    
    def decrire_image(self, image: Image.Image) -> str:
        """G√©n√®re une description en fran√ßais d'une image.
        
        Args:
            image: Image PIL √† d√©crire
            
        Returns:
            Description en fran√ßais
        """
        # G√©n√©ration de la l√©gende en anglais avec BLIP
        inputs = self.processor_blip(image, return_tensors="pt").to(self.peripherique)
        
        with torch.no_grad():
            outputs = self.modele_blip.generate(
                **inputs,
                max_length=self.longueur_max,
                min_length=self.longueur_min,
                num_beams=self.num_beams,
                length_penalty=1.0,
                repetition_penalty=1.5,
                temperature=self.temperature,
                do_sample=True,
                top_k=50,
                top_p=0.95,
            )
        
        caption_en = self.processor_blip.decode(outputs[0], skip_special_tokens=True)
        
        # Traduction en fran√ßais
        inputs_trad = self.tokenizer_trad(caption_en, return_tensors="pt").to(
            self.peripherique
        )
        
        with torch.no_grad():
            translated = self.modele_trad.generate(**inputs_trad)
        
        caption_fr = self.tokenizer_trad.decode(translated[0], skip_special_tokens=True)
        
        return caption_fr
    
    def encoder_image(self, image: Union[Image.Image, str, Path]) -> tuple[np.ndarray, str]:
        """Encode une image en vecteur via sa description textuelle.
        
        Args:
            image: Image PIL, chemin vers fichier image, ou Path
            
        Returns:
            Tuple (embedding normalis√©, description en fran√ßais)
        """
        # Charger l'image si c'est un chemin
        if isinstance(image, (str, Path)):
            image = Image.open(image).convert("RGB")
        
        # G√©n√©rer la description
        description = self.decrire_image(image)
        
        # Encoder la description avec le m√™me encodeur que les messages
        embedding = self.encodeur_texte.encoder([description])[0]
        
        return embedding, description
    
    def encoder_images_batch(
        self, 
        images: List[Union[Image.Image, str, Path]],
        show_progress: bool = True,
    ) -> List[tuple[np.ndarray, str]]:
        """Encode un lot d'images.
        
        Args:
            images: Liste d'images (PIL ou chemins)
            show_progress: Afficher la progression
            
        Returns:
            Liste de tuples (embedding, description)
        """
        resultats = []
        
        for i, image in enumerate(images):
            if show_progress:
                print(f"   Traitement image {i+1}/{len(images)}...", end="\r")
            
            try:
                embedding, description = self.encoder_image(image)
                resultats.append((embedding, description))
            except Exception as e:
                print(f"\n‚ö†Ô∏è  Erreur lors du traitement de l'image {i+1}: {e}")
                # Retourner un embedding nul et une description vide
                embedding_nul = np.zeros(self.encodeur_texte.dimension_embedding)
                resultats.append((embedding_nul, ""))
        
        if show_progress:
            print()  # Nouvelle ligne apr√®s la progression
        
        return resultats
    
    @property
    def dimension_embedding(self) -> int:
        """Dimension des embeddings (m√™me dimension que l'encodeur de texte)."""
        return self.encodeur_texte.dimension_embedding


def creer_encodeur_image(encodeur_texte, parametres: object) -> EncodeurImage:
    """Factory pour cr√©er un encodeur d'images depuis les param√®tres.
    
    Args:
        encodeur_texte: Instance d'EncodeurTexte (partag√© avec les messages)
        parametres: Objet de param√®tres exposant les attributs de configuration
        
    Returns:
        EncodeurImage configur√©
    """
    return EncodeurImage(
        encodeur_texte=encodeur_texte,
        preference_peripherique=getattr(parametres, "PERIPHERIQUE_EMBEDDING", "auto"),
        longueur_min_description=getattr(parametres, "LONGUEUR_MIN_DESCRIPTION_IMAGE", 30),
        longueur_max_description=getattr(parametres, "LONGUEUR_MAX_DESCRIPTION_IMAGE", 150),
        num_beams=getattr(parametres, "NUM_BEAMS_DESCRIPTION_IMAGE", 15),
        temperature=getattr(parametres, "TEMPERATURE_DESCRIPTION_IMAGE", 0.3),
    )


