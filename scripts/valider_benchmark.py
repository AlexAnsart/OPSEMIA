#!/usr/bin/env python3
"""Script de validation du systÃ¨me de benchmark.

VÃ©rifie que tous les composants sont prÃªts avant d'exÃ©cuter le benchmark complet.
"""

import os
import sys
from pathlib import Path

# Ajouter le rÃ©pertoire racine au path
racine_projet = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(racine_projet))

# Charger les variables d'environnement depuis .env
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("âš ï¸  python-dotenv non installÃ©, impossible de charger .env automatiquement")
    print("   Installation: pip install python-dotenv")


def verifier_fichiers():
    """VÃ©rifie que tous les fichiers nÃ©cessaires existent."""
    print("ğŸ“ VÃ©rification des fichiers...")
    
    fichiers_requis = [
        racine_projet / "Cas" / "Cas4" / "sms.csv",
        racine_projet / "Cas" / "Cas4" / "images.csv",
        racine_projet / "scripts" / "donnees_benchmark_opsemia.py",
        racine_projet / "scripts" / "benchmark_complet_opsemia.py",
    ]
    
    dossiers_requis = [
        racine_projet / "Cas" / "Cas4" / "Images",
        racine_projet / "Docs Projet",
    ]
    
    erreurs = []
    
    for fichier in fichiers_requis:
        if not fichier.exists():
            erreurs.append(f"âŒ Fichier manquant: {fichier}")
        else:
            print(f"   âœ“ {fichier.name}")
    
    for dossier in dossiers_requis:
        if not dossier.exists():
            erreurs.append(f"âŒ Dossier manquant: {dossier}")
        else:
            nb_fichiers = len(list(dossier.glob("*")))
            print(f"   âœ“ {dossier.name}/ ({nb_fichiers} fichiers)")
    
    return erreurs


def verifier_dependances():
    """VÃ©rifie que les dÃ©pendances Python sont installÃ©es."""
    print("\nğŸ“¦ VÃ©rification des dÃ©pendances...")
    
    dependances = [
        ("numpy", "numpy"),
        ("sentence_transformers", "sentence-transformers"),
        ("PIL", "Pillow"),
        ("transformers", "transformers"),
        ("requests", "requests"),
        ("chromadb", "chromadb"),
    ]
    
    erreurs = []
    
    for module, package in dependances:
        try:
            __import__(module)
            print(f"   âœ“ {package}")
        except ImportError:
            erreurs.append(f"âŒ Package manquant: {package}")
            erreurs.append(f"   Installation: pip install {package}")
    
    return erreurs


def verifier_configuration():
    """VÃ©rifie la configuration."""
    print("\nâš™ï¸  VÃ©rification de la configuration...")
    
    erreurs = []
    
    # VÃ©rifier fichier .env
    chemin_env = racine_projet / ".env"
    print(f"\n   ğŸ” DÃ©bogage .env:")
    print(f"      Racine projet: {racine_projet}")
    print(f"      Fichier .env attendu: {chemin_env}")
    print(f"      Fichier .env existe: {chemin_env.exists()}")
    
    if chemin_env.exists():
        print(f"      Taille .env: {chemin_env.stat().st_size} octets")
        # Lire le contenu (masquÃ©)
        try:
            with open(chemin_env, 'r') as f:
                contenu = f.read()
                if "DEEPINFRA_TOKEN" in contenu:
                    print(f"      âœ“ Variable DEEPINFRA_TOKEN trouvÃ©e dans .env")
                else:
                    print(f"      âš ï¸  Variable DEEPINFRA_TOKEN absente du .env")
        except Exception as e:
            print(f"      âš ï¸  Erreur lecture .env: {e}")
    
    # VÃ©rifier clÃ© API
    deepinfra_token = os.getenv("DEEPINFRA_TOKEN")
    print(f"      os.getenv('DEEPINFRA_TOKEN'): {'***' + deepinfra_token[-10:] if deepinfra_token else 'None'}")
    
    if not deepinfra_token:
        erreurs.append("âš ï¸  DEEPINFRA_TOKEN non configurÃ© dans les variables d'environnement")
        erreurs.append("   CrÃ©er un fichier .env Ã  la racine avec: DEEPINFRA_TOKEN=votre_clÃ©")
        erreurs.append(f"   Ou dÃ©finir la variable: export DEEPINFRA_TOKEN=votre_clÃ©")
    else:
        cle_masquee = deepinfra_token[:10] + "..." + deepinfra_token[-5:] if len(deepinfra_token) > 15 else "***"
        print(f"   âœ“ DEEPINFRA_TOKEN configurÃ©: {cle_masquee}")
    
    # VÃ©rifier imports projet
    try:
        from config.settings import obtenir_parametres
        parametres = obtenir_parametres()
        print(f"   âœ“ Configuration OPSEMIA chargÃ©e")
        print(f"      ModÃ¨le actuel: {parametres.ID_MODELE_EMBEDDING}")
    except Exception as e:
        erreurs.append(f"âŒ Erreur chargement config: {e}")
    
    return erreurs


def verifier_dataset_benchmark():
    """VÃ©rifie le dataset de benchmark."""
    print("\nğŸ¯ VÃ©rification du dataset de benchmark...")
    
    erreurs = []
    
    try:
        from scripts.donnees_benchmark_opsemia import (
            obtenir_requetes_messages,
            obtenir_requetes_images,
        )
        
        requetes_messages = obtenir_requetes_messages()
        requetes_images = obtenir_requetes_images()
        
        print(f"   âœ“ {len(requetes_messages)} requÃªtes messages")
        print(f"   âœ“ {len(requetes_images)} requÃªtes images")
        
        # VÃ©rifier requÃªtes images complÃ©tÃ©es
        requetes_non_completees = sum(
            1 for req, _, _ in requetes_images 
            if "[REQUETE_A_COMPLETER]" in req
        )
        
        if requetes_non_completees > 0:
            erreurs.append(f"âš ï¸  {requetes_non_completees} requÃªtes images non complÃ©tÃ©es")
            erreurs.append("   Ã‰diter scripts/donnees_benchmark_opsemia.py")
        else:
            print(f"   âœ“ Toutes les requÃªtes images sont complÃ©tÃ©es")
        
        # VÃ©rifier format
        for i, (req, ids, diff) in enumerate(requetes_messages[:3]):
            if not req or not ids or diff not in ["facile", "moyen", "difficile"]:
                erreurs.append(f"âŒ RequÃªte message {i+1} mal formatÃ©e")
        
        print(f"   âœ“ Format du dataset valide")
        
    except Exception as e:
        erreurs.append(f"âŒ Erreur chargement dataset: {e}")
    
    return erreurs


def verifier_espace_disque():
    """VÃ©rifie l'espace disque disponible."""
    print("\nğŸ’¾ VÃ©rification de l'espace disque...")
    
    erreurs = []
    
    try:
        import shutil
        stat = shutil.disk_usage(racine_projet)
        
        gb_disponible = stat.free / (1024**3)
        gb_total = stat.total / (1024**3)
        
        print(f"   Espace disponible: {gb_disponible:.1f} GB / {gb_total:.1f} GB")
        
        if gb_disponible < 10:
            erreurs.append(f"âš ï¸  Espace disque faible: {gb_disponible:.1f} GB")
            erreurs.append("   RecommandÃ©: au moins 10 GB pour les modÃ¨les")
        else:
            print(f"   âœ“ Espace disque suffisant")
        
    except Exception as e:
        erreurs.append(f"âš ï¸  Impossible de vÃ©rifier l'espace disque: {e}")
    
    return erreurs


def afficher_estimation_temps():
    """Affiche une estimation du temps d'exÃ©cution."""
    print("\nâ±ï¸  Estimation du temps d'exÃ©cution:")
    print("   - Calcul des durÃ©es: ~5-10 minutes")
    print("   - ModÃ¨les locaux (Ã—3): ~15-30 minutes")
    print("   - Qwen3-8B via API: ~20-30 minutes")
    print("   - Total estimÃ©: ~40-70 minutes")
    print("\n   ğŸ’¡ Conseils:")
    print("   - ExÃ©cuter pendant une pause")
    print("   - VÃ©rifier la connexion Internet (pour API)")
    print("   - GPU recommandÃ© pour les modÃ¨les locaux")


def main():
    """Fonction principale de validation."""
    print("="*80)
    print("VALIDATION DU SYSTÃˆME DE BENCHMARK OPSEMIA")
    print("="*80)
    
    toutes_erreurs = []
    
    # VÃ©rifications
    toutes_erreurs.extend(verifier_fichiers())
    toutes_erreurs.extend(verifier_dependances())
    toutes_erreurs.extend(verifier_configuration())
    toutes_erreurs.extend(verifier_dataset_benchmark())
    toutes_erreurs.extend(verifier_espace_disque())
    
    # Afficher estimation
    afficher_estimation_temps()
    
    # RÃ©sumÃ©
    print("\n" + "="*80)
    if toutes_erreurs:
        print("âŒ VALIDATION Ã‰CHOUÃ‰E")
        print("="*80)
        print("\nErreurs et avertissements:")
        for erreur in toutes_erreurs:
            print(f"  {erreur}")
        print("\nğŸ’¡ Corrigez les erreurs avant d'exÃ©cuter le benchmark.")
    else:
        print("âœ… VALIDATION RÃ‰USSIE")
        print("="*80)
        print("\nğŸš€ Le systÃ¨me est prÃªt pour le benchmark!")
        print("\nPour lancer le benchmark complet:")
        print("   python scripts/benchmark_complet_opsemia.py")
        print("\nPour visualiser le dataset:")
        print("   python scripts/afficher_dataset_benchmark.py")
    
    print("="*80)
    
    return len(toutes_erreurs)


if __name__ == "__main__":
    exit_code = main()
    sys.exit(0 if exit_code == 0 else 1)

