#!/usr/bin/env python3
"""Benchmark complet pour OPSEMIA : Calcul de dur√©es et qualit√© de recherche.

Ce script effectue :
1. Calcul des dur√©es d'encodage pour tous les mod√®les (messages et images)
2. Benchmark de qualit√© sur 100 requ√™tes (90 messages + 10 images)
3. G√©n√©ration d'un rapport markdown complet

Mod√®les test√©s :
- Jina-v3 (jinaai/jina-embeddings-v3)
- Solon-large (OrdalieTech/Solon-embeddings-large-0.1)
- BGE-M3 (BAAI/bge-m3)
- Qwen3-8B (via API DeepInfra - estimation pour dur√©es)
- BLIP (pour description d'images)
"""

import argparse
import io
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import requests
import torch
from dotenv import load_dotenv
from PIL import Image
from sentence_transformers import SentenceTransformer

# Charger les variables d'environnement depuis .env
load_dotenv()

# Forcer UTF-8 pour la sortie console
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
if sys.stderr.encoding != 'utf-8':
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Ajouter le r√©pertoire racine au path
racine_projet = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(racine_projet))

from config.settings import obtenir_parametres, TOP_K_BENCHMARK
from src.backend.database.vector_db import BaseVectorielle
from src.backend.models.image_encoder import EncodeurImage
from src.backend.models.text_encoder import EncodeurTexte
from src.backend.parsers.message_extractor import parser_sms_depuis_csv
from src.backend.parsers.image_extractor import parser_images_depuis_csv
from scripts.donnees_benchmark_opsemia import (
    obtenir_requetes_messages,
    obtenir_requetes_images,
    obtenir_toutes_requetes,
)


# ============================================================================
# CONFIGURATION
# ============================================================================

MODELES_TEXTE = [
    {
        "id": "jinaai/jina-embeddings-v3",
        "nom": "Jina-v3",
        "params_millions": 137,  # Estimation
        "dimensions": 1024,
        "local": True,
    },
    {
        "id": "OrdalieTech/Solon-embeddings-large-0.1",
        "nom": "Solon-large",
        "params_millions": 335,  # Estimation
        "dimensions": 1024,
        "local": True,
    },
    {
        "id": "BAAI/bge-m3",
        "nom": "BGE-M3",
        "params_millions": 568,  # Estimation
        "dimensions": 1024,
        "local": True,
    },
    {
        "id": "Qwen/Qwen3-Embedding-8B",
        "nom": "Qwen3-8B",
        "params_millions": 8000,
        "dimensions": 8192,
        "local": False,  # Via API
    },
]

# Chemins
CHEMIN_CSV_MESSAGES = racine_projet / "Cas" / "Cas4" / "sms.csv"
CHEMIN_CSV_IMAGES = racine_projet / "Cas" / "Cas4" / "images.csv"
DOSSIER_RACINE_CAS4 = racine_projet / "Cas" / "Cas4"  # Dossier parent pour r√©soudre les chemins relatifs du CSV
DOSSIER_IMAGES = racine_projet / "Cas" / "Cas4" / "Images"  # Pour compter les fichiers
CHEMIN_OUTPUT_MD = racine_projet / "Docs Projet" / f"BENCHMARK_RESULTATS_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

# Base de donn√©es temporaire pour benchmark
CHEMIN_DB_BENCHMARK = racine_projet / "data" / "benchmark_temp"

# Cl√© API DeepInfra
DEEPINFRA_API_KEY = os.getenv("DEEPINFRA_TOKEN")


# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def calculer_similarite_cosine(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """Calcule la similarit√© cosinus entre deux vecteurs normalis√©s."""
    return float(np.dot(vec1, vec2))


def precision_at_k(resultats_ids: List[str], pertinents_ids: List[str], k: int) -> float:
    """Calcule la Precision@K."""
    if k == 0:
        return 0.0
    top_k = resultats_ids[:k]
    nb_pertinents = len([doc_id for doc_id in top_k if doc_id in pertinents_ids])
    return nb_pertinents / k


def recall_at_k(resultats_ids: List[str], pertinents_ids: List[str], k: int) -> float:
    """Calcule le Recall@K."""
    if len(pertinents_ids) == 0:
        return 0.0
    top_k = resultats_ids[:k]
    nb_pertinents = len([doc_id for doc_id in top_k if doc_id in pertinents_ids])
    return nb_pertinents / len(pertinents_ids)


def mean_reciprocal_rank(resultats_ids: List[str], pertinents_ids: List[str]) -> float:
    """Calcule le Mean Reciprocal Rank (MRR)."""
    for i, doc_id in enumerate(resultats_ids, 1):
        if doc_id in pertinents_ids:
            return 1.0 / i
    return 0.0


def est_recherche_reussie(resultats_ids: List[str], pertinents_ids: List[str], k: int) -> bool:
    """D√©termine si une recherche est r√©ussie.
    
    Une recherche est r√©ussie si au moins 1 document pertinent est dans le top K.
    
    Args:
        resultats_ids: Liste des IDs des r√©sultats retourn√©s
        pertinents_ids: Liste des IDs des documents pertinents attendus
        k: Nombre de premiers r√©sultats √† consid√©rer (g√©n√©ralement TOP_K_BENCHMARK)
        
    Returns:
        True si au moins 1 document pertinent est dans les K premiers r√©sultats
    """
    top_k = resultats_ids[:k]
    return any(doc_id in pertinents_ids for doc_id in top_k)


# ============================================================================
# ENCODAGE VIA API (QWEN3)
# ============================================================================

def encoder_texte_via_api(textes: List[str], modele: str = "Qwen/Qwen3-Embedding-8B") -> List[List[float]]:
    """Encode des textes via l'API DeepInfra.
    
    Args:
        textes: Liste de textes √† encoder
        modele: ID du mod√®le √† utiliser
        
    Returns:
        Liste d'embeddings
    """
    if not DEEPINFRA_API_KEY:
        raise ValueError("DEEPINFRA_TOKEN non configur√© dans les variables d'environnement")
    
    url = "https://api.deepinfra.com/v1/openai/embeddings"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {DEEPINFRA_API_KEY}"
    }
    
    embeddings = []
    for texte in textes:
        payload = {
            "input": texte,
            "model": modele,
            "encoding_format": "float"
        }
        
        response = requests.post(url, json=payload, headers=headers)
        result = response.json()
        
        if "data" in result and len(result["data"]) > 0:
            embeddings.append(result["data"][0]["embedding"])
        else:
            raise ValueError(f"Erreur API: {result}")
    
    return embeddings


# ============================================================================
# CALCUL DES DUR√âES D'ENCODAGE
# ============================================================================

def calculer_durees_encodage_texte() -> Dict[str, Dict]:
    """Calcule les dur√©es d'encodage pour chaque mod√®le de texte.
    
    Approche simplifi√©e :
    1. Encoder un court texte (5-10 tokens)
    2. Calculer dur√©e par token
    3. Extrapoler pour 1 message (10 tokens), 100k messages
    
    Returns:
        Dictionnaire avec les dur√©es par mod√®le
    """
    print("\n" + "="*80)
    print("CALCUL DES DUR√âES D'ENCODAGE - MOD√àLES TEXTE")
    print("="*80)
    
    resultats = {}
    
    # Texte de test court (environ 5 tokens)
    texte_test = "Ceci est un test"
    NB_TOKENS_TEST = 5
    NB_TOKENS_PAR_MESSAGE = 10  # Estimation moyenne d'un message
    NB_TESTS = 5  # R√©p√©ter pour moyenne plus stable
    
    for modele_info in MODELES_TEXTE:
        print(f"\nüìä Mod√®le: {modele_info['nom']}")
        print(f"   ID: {modele_info['id']}")
        print(f"   Param√®tres: {modele_info['params_millions']}M")
        
        if modele_info['local']:
            # Mod√®le local
            print("   ‚è≥ Chargement du mod√®le...")
            debut_chargement = time.time()
            try:
                try:
                    modele = SentenceTransformer(modele_info['id'], trust_remote_code=True)
                except:
                    modele = SentenceTransformer(modele_info['id'])
                duree_chargement = time.time() - debut_chargement
                print(f"   ‚úì Charg√© en {duree_chargement:.2f}s")
                
                # Mesurer dur√©e d'encodage (moyenne sur plusieurs tests)
                print("   ‚è≥ Test d'encodage...")
                durees = []
                for i in range(NB_TESTS):
                    debut = time.time()
                    _ = modele.encode([texte_test], normalize_embeddings=True, show_progress_bar=False)
                    durees.append(time.time() - debut)
                
                duree_moyenne = np.mean(durees)
                duree_par_token = duree_moyenne / NB_TOKENS_TEST
                
                # Extrapolations
                duree_1_message = duree_par_token * NB_TOKENS_PAR_MESSAGE
                duree_100k_messages = duree_1_message * 100_000
                
                resultats[modele_info['nom']] = {
                    "duree_chargement_s": duree_chargement,
                    "duree_par_token_ms": duree_par_token * 1000,
                    "duree_1_message_ms": duree_1_message * 1000,
                    "debit_msg_par_s": 1 / duree_1_message if duree_1_message > 0 else 0,
                    "estimation_100k_messages_h": duree_100k_messages / 3600,
                }
                
                print(f"   ‚è±Ô∏è  Par token: {duree_par_token*1000:.2f}ms")
                print(f"   ‚è±Ô∏è  Par message ({NB_TOKENS_PAR_MESSAGE} tokens): {duree_1_message*1000:.2f}ms")
                print(f"   ‚è±Ô∏è  D√©bit: {1/duree_1_message:.1f} msg/s")
                print(f"   üìà Estimation 100k messages: {duree_100k_messages/3600:.1f}h")
                
            except Exception as e:
                print(f"   ‚ùå ERREUR lors du test du mod√®le: {type(e).__name__}: {str(e)}")
                print(f"   ‚ö†Ô∏è  Le mod√®le sera ignor√© dans les r√©sultats")
                resultats[modele_info['nom']] = {
                    "erreur": str(e),
                    "duree_chargement_s": 0,
                    "duree_par_token_ms": 0,
                    "duree_1_message_ms": 0,
                    "debit_msg_par_s": 0,
                    "estimation_100k_messages_h": 0,
                }
            finally:
                # Lib√©rer la m√©moire
                if 'modele' in locals():
                    del modele
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                import gc
                gc.collect()
            
        else:
            # Mod√®le via API - Estimation bas√©e sur les autres mod√®les
            print("   üåê Mod√®le via API - Estimation bas√©e sur le nombre de param√®tres")
            
            # Facteur bas√© sur le ratio de param√®tres
            facteur = modele_info['params_millions'] / MODELES_TEXTE[2]['params_millions']  # BGE-M3 r√©f√©rence
            
            # Estimation (en supposant ~2ms/token pour BGE-M3)
            duree_par_token_estimee = 2 * facteur  # ms
            duree_1_message_estimee = duree_par_token_estimee * NB_TOKENS_PAR_MESSAGE
            duree_100k_estimee = (duree_1_message_estimee / 1000) * 100_000
            
            resultats[modele_info['nom']] = {
                "duree_chargement_s": "N/A (API)",
                "duree_par_token_ms": f"~{duree_par_token_estimee:.1f}",
                "duree_1_message_ms": f"~{duree_1_message_estimee:.1f}",
                "debit_msg_par_s": f"~{1000/duree_1_message_estimee:.1f}",
                "estimation_100k_messages_h": f"~{duree_100k_estimee/3600:.1f}",
                "note": f"Estimation bas√©e sur ratio de param√®tres (facteur {facteur:.1f}x vs BGE-M3)",
            }
            
            print(f"   üìä Par token: ~{duree_par_token_estimee:.1f}ms")
            print(f"   üìä Par message ({NB_TOKENS_PAR_MESSAGE} tokens): ~{duree_1_message_estimee:.1f}ms")
            print(f"   üìä D√©bit: ~{1000/duree_1_message_estimee:.1f} msg/s")
            print(f"   üìà Estimation 100k messages: ~{duree_100k_estimee/3600:.1f}h")
            print(f"   ‚ÑπÔ∏è  {resultats[modele_info['nom']]['note']}")
    
    return resultats


def calculer_durees_encodage_images() -> Dict[str, any]:
    """Calcule les dur√©es d'encodage pour les images (BLIP + encodage texte).
    
    Approche simplifi√©e :
    1. Encoder 1 image de test
    2. Mesurer description + encodage
    3. Extrapoler pour 1000 images
    
    Returns:
        Dictionnaire avec les dur√©es
    """
    print("\n" + "="*80)
    print("CALCUL DES DUR√âES D'ENCODAGE - IMAGES (BLIP)")
    print("="*80)
    
    # Charger un encodeur de texte pour BLIP
    parametres = obtenir_parametres()
    encodeur_texte = EncodeurTexte(
        id_modele=parametres.ID_MODELE_EMBEDDING,
        preference_peripherique=parametres.PERIPHERIQUE_EMBEDDING
    )
    
    # Cr√©er encodeur d'images
    print("‚è≥ Chargement BLIP...")
    debut_chargement = time.time()
    encodeur_image = EncodeurImage(
        encodeur_texte=encodeur_texte,
        preference_peripherique=parametres.PERIPHERIQUE_EMBEDDING,
    )
    duree_chargement = time.time() - debut_chargement
    print(f"‚úì BLIP charg√© en {duree_chargement:.2f}s")
    
    # Charger images de test (passer le dossier parent pour r√©soudre les chemins relatifs)
    images_csv = parser_images_depuis_csv(CHEMIN_CSV_IMAGES, DOSSIER_RACINE_CAS4)
    images_valides = [img for img in images_csv if img["existe"]]
    
    if len(images_valides) == 0:
        print("‚ö†Ô∏è  Aucune image disponible pour les tests de dur√©e")
        print("   Les estimations seront bas√©es sur des valeurs typiques (~30s/image)")
        
        return {
            "duree_chargement_s": duree_chargement,
            "duree_description_moyenne_s": 28.0,  # Valeur typique BLIP (~28s)
            "duree_encodage_desc_moyenne_ms": 2000,  # Valeur typique (~2s)
            "duree_totale_moyenne_s": 30.0,
            "duree_min_s": "N/A",
            "duree_max_s": "N/A",
            "estimation_1000_images_h": 30.0 * 1000 / 3600,
            "note": "Estimation bas√©e sur valeurs typiques (aucune image disponible)",
        }
    
    # Tester sur 3 images pour moyenne
    nb_tests = min(3, len(images_valides))
    print(f"\n‚è≥ Test sur {nb_tests} images...")
    
    try:
        temps_description = []
        temps_encodage = []
        
        for img_info in images_valides[:nb_tests]:
            chemin = Path(img_info["chemin_absolu"])
            image_pil = Image.open(chemin).convert("RGB")
            
            # Description
            debut = time.time()
            description = encodeur_image.decrire_image(image_pil)
            temps_description.append(time.time() - debut)
            
            # Encodage
            debut = time.time()
            _ = encodeur_texte.encoder([description])
            temps_encodage.append(time.time() - debut)
        
        duree_desc_moyenne = np.mean(temps_description)
        duree_enc_moyenne = np.mean(temps_encodage)
        duree_totale_moyenne = duree_desc_moyenne + duree_enc_moyenne
        
        resultats = {
            "duree_chargement_s": duree_chargement,
            "duree_description_moyenne_s": duree_desc_moyenne,
            "duree_encodage_desc_moyenne_ms": duree_enc_moyenne * 1000,
            "duree_totale_moyenne_s": duree_totale_moyenne,
            "duree_min_s": min(temps_description) + min(temps_encodage),
            "duree_max_s": max(temps_description) + max(temps_encodage),
            "estimation_1000_images_h": (duree_totale_moyenne * 1000) / 3600,
        }
        
        print(f"\nüìä R√©sultats ({nb_tests} images test√©es):")
        print(f"   ‚è±Ô∏è  Description moyenne: {duree_desc_moyenne:.2f}s")
        print(f"   ‚è±Ô∏è  Encodage description: {duree_enc_moyenne*1000:.2f}ms")
        print(f"   ‚è±Ô∏è  Total moyen/image: {duree_totale_moyenne:.2f}s")
        print(f"   üìà Estimation 1000 images: {resultats['estimation_1000_images_h']:.1f}h")
        
        return resultats
        
    except Exception as e:
        print(f"\n‚ùå ERREUR lors du test des images: {type(e).__name__}: {str(e)}")
        print("   ‚ö†Ô∏è  Utilisation de valeurs par d√©faut")
        return {
            "duree_chargement_s": duree_chargement,
            "duree_description_moyenne_s": 28.0,
            "duree_encodage_desc_moyenne_ms": 2000,
            "duree_totale_moyenne_s": 30.0,
            "duree_min_s": "N/A",
            "duree_max_s": "N/A",
            "estimation_1000_images_h": 30.0 * 1000 / 3600,
            "note": f"Erreur: {str(e)}",
        }


# ============================================================================
# BENCHMARK DE QUALIT√â
# ============================================================================

def indexer_dataset_benchmark(modele_info: Dict, encodeur_texte, encodeur_image=None, force_reindex: bool = False) -> str:
    """Indexe le dataset complet pour un mod√®le donn√©.
    
    Args:
        modele_info: Informations du mod√®le
        encodeur_texte: Encodeur de texte (local ou None si API)
        encodeur_image: Encodeur d'images (optionnel)
        force_reindex: Si True, force la r√©indexation m√™me si la collection existe
        
    Returns:
        Nom de la collection cr√©√©e
    """
    nom_collection = f"benchmark_{modele_info['nom'].lower().replace('-', '_')}"
    
    # Cr√©er la base vectorielle temporaire
    db = BaseVectorielle(chemin_persistance=CHEMIN_DB_BENCHMARK)
    
    # V√©rifier si la collection existe d√©j√† (utiliser l'API ChromaDB directement)
    collections_existantes = [col.name for col in db.client.list_collections()]
    
    if nom_collection in collections_existantes and not force_reindex:
        nb_docs_existants = db.compter_documents(nom_collection)
        print(f"\n‚ôªÔ∏è  Collection {nom_collection} d√©j√† index√©e (r√©utilisation)")
        print(f"   Collection: {nom_collection}")
        print(f"   üìä {nb_docs_existants} documents en cache")
        if not modele_info['local']:
            print(f"   üí∞ √âconomie : pas de nouveaux appels API DeepInfra")
        print(f"   üí° Utilisez --force pour r√©indexer")
        return nom_collection
    
    print(f"\nüì¶ Indexation dataset pour {modele_info['nom']}...")
    print(f"   Collection: {nom_collection}")
    
    # Supprimer collection existante si force_reindex
    if nom_collection in collections_existantes:
        print(f"   üóëÔ∏è  Suppression de l'ancienne collection...")
        db.supprimer_collection(nom_collection)
    
    # Parser les messages
    print("   üìÑ Parsing messages...")
    messages = parser_sms_depuis_csv(CHEMIN_CSV_MESSAGES)
    print(f"   ‚úì {len(messages)} messages pars√©s")
    
    # Encoder les messages
    print(f"   üß† Encodage {len(messages)} messages...")
    textes = [msg.get("message", "") or "" for msg in messages]
    
    # DEBUG: Afficher les 3 premiers messages pour v√©rifier
    print(f"   üîç DEBUG - V√©rification du contenu:")
    for i in range(min(3, len(messages))):
        print(f"      msg_{i:03d}: {textes[i][:60]}...")
    
    debut_encodage = time.time()
    if modele_info['local']:
        embeddings_msgs = encodeur_texte.encoder(textes, taille_lot=32)
    else:
        # Via API (Qwen3) - Encoder par lots avec logs de progression
        print("   üåê Encodage via API DeepInfra (cela peut prendre du temps)...")
        embeddings_bruts = []
        taille_lot_api = 50  # Encoder par lots de 50 pour √©viter les timeouts
        
        for i in range(0, len(textes), taille_lot_api):
            lot = textes[i:i+taille_lot_api]
            debut_lot = time.time()
            
            try:
                embeddings_lot = encoder_texte_via_api(lot, modele_info['id'])
                embeddings_bruts.extend(embeddings_lot)
                
                duree_lot = time.time() - debut_lot
                progression = min(i + taille_lot_api, len(textes))
                pct = (progression / len(textes)) * 100
                temps_ecoule = time.time() - debut_encodage
                temps_restant_estime = (temps_ecoule / progression) * (len(textes) - progression) if progression > 0 else 0
                
                print(f"      [{progression}/{len(textes)}] {pct:.1f}% - {duree_lot:.2f}s pour {len(lot)} msgs - "
                      f"Temps √©coul√©: {temps_ecoule:.1f}s - Restant estim√©: {temps_restant_estime:.1f}s")
            except Exception as e:
                print(f"      ‚ùå Erreur lors de l'encodage du lot {i}-{i+len(lot)}: {e}")
                raise
        
        embeddings_msgs = np.array(embeddings_bruts)
    
    duree_encodage = time.time() - debut_encodage
    print(f"   ‚úì {len(embeddings_msgs)} embeddings g√©n√©r√©s en {duree_encodage:.1f}s ({len(messages)/duree_encodage:.1f} msg/s)")
    
    # Stocker messages - utiliser "id" car c'est ce que le parser retourne (pas "id_message")
    # Assurer le format avec padding (msg_001, msg_002, etc.)
    ids_msgs = []
    for i, msg in enumerate(messages):
        msg_id = msg.get("id")
        if not msg_id:
            # Fallback avec padding √† 3 chiffres
            msg_id = f"msg_{i:03d}"
        ids_msgs.append(msg_id)
    
    metadonnees_msgs = [
        {
            "id": ids_msgs[i],
            "type": "message",
            "contact": msg.get("contact_name", ""),
            "canal": msg.get("app", ""),
        }
        for i, msg in enumerate(messages)
    ]
    
    # Log des premiers IDs pour debug
    print(f"   üîç Debug - Premiers IDs de messages index√©s:")
    print(f"      {ids_msgs[:5]}")
    
    db.ajouter_messages(
        nom_collection=nom_collection,
        ids=ids_msgs,
        embeddings=embeddings_msgs.tolist(),
        metadonnees=metadonnees_msgs,
        documents=textes,
    )
    
    # Indexer images si encodeur fourni
    if encodeur_image:
        print("   üñºÔ∏è  Indexation images...")
        images = parser_images_depuis_csv(CHEMIN_CSV_IMAGES, DOSSIER_RACINE_CAS4)
        images_valides = [img for img in images if img["existe"]][:10]
        
        debut_images = time.time()
        for i, img_info in enumerate(images_valides, 1):
            debut_img = time.time()
            chemin = Path(img_info["chemin_absolu"])
            image_pil = Image.open(chemin).convert("RGB")
            
            # G√©n√©rer description et encoder
            description = encodeur_image.decrire_image(image_pil)
            if modele_info['local']:
                embedding = encodeur_texte.encoder([description])[0]
            else:
                embedding_brut = encoder_texte_via_api([description], modele_info['id'])[0]
                embedding = np.array(embedding_brut)
            
            # Stocker
            db.ajouter_messages(
                nom_collection=nom_collection,
                ids=[f"img_{i-1}"],  # img_0, img_1, ..., img_9
                embeddings=[embedding.tolist()],
                metadonnees=[{
                    "id": f"img_{i-1}",
                    "type": "image",
                    "nom_image": img_info["nom_image"],
                    "description": description,
                }],
                documents=[description],
            )
            
            duree_img = time.time() - debut_img
            pct = (i / len(images_valides)) * 100
            temps_ecoule = time.time() - debut_images
            temps_restant = (temps_ecoule / i) * (len(images_valides) - i)
            
            print(f"      [{i}/{len(images_valides)}] {pct:.1f}% - {duree_img:.2f}s - "
                  f"√âcoul√©: {temps_ecoule:.1f}s - Restant: {temps_restant:.1f}s")
        
        duree_totale = time.time() - debut_images
        duree_par_image = duree_totale / len(images_valides) if len(images_valides) > 0 else 0
        print(f"   ‚úì {len(images_valides)} images index√©es en {duree_totale:.1f}s ({duree_par_image:.2f}s/image)")
    else:
        print(f"   ‚ö†Ô∏è  Aucune image valide trouv√©e, indexation uniquement des messages")
    
    print(f"   ‚úÖ Indexation termin√©e: {nom_collection}")
    return nom_collection


def evaluer_modele_benchmark(modele_info: Dict, nom_collection: str) -> Dict:
    """√âvalue un mod√®le sur les 100 requ√™tes de benchmark.
    
    Args:
        modele_info: Informations du mod√®le
        nom_collection: Nom de la collection index√©e
        
    Returns:
        R√©sultats du benchmark
    """
    print(f"\nüéØ √âvaluation {modele_info['nom']} sur 100 requ√™tes...")
    
    # Charger encodeur
    if modele_info['local']:
        try:
            encodeur = SentenceTransformer(modele_info['id'], trust_remote_code=True)
        except:
            encodeur = SentenceTransformer(modele_info['id'])
    else:
        encodeur = None  # API
    
    # Base vectorielle
    db = BaseVectorielle(chemin_persistance=CHEMIN_DB_BENCHMARK)
    
    # M√©triques
    metriques = {
        "precision@1": [],
        "precision@3": [],
        f"precision@{TOP_K_BENCHMARK}": [],
        f"recall@{TOP_K_BENCHMARK}": [],
        "mrr": [],
        "reussites": [],  # 1 si r√©ussite, 0 sinon
    }
    
    metriques_par_type = {
        "message": {f"precision@{TOP_K_BENCHMARK}": [], f"recall@{TOP_K_BENCHMARK}": [], "mrr": [], "reussites": []},
        "image": {f"precision@{TOP_K_BENCHMARK}": [], f"recall@{TOP_K_BENCHMARK}": [], "mrr": [], "reussites": []},
    }
    
    # R√©cup√©rer toutes les requ√™tes
    requetes = obtenir_toutes_requetes()
    
    # V√©rifier le nombre de documents dans la collection
    nb_docs = db.compter_documents(nom_collection)
    print(f"   üìä Collection contient {nb_docs} documents")
    print(f"   üìè Crit√®re: au moins 1 ID attendu dans le top {TOP_K_BENCHMARK} r√©sultats")
    
    debut_eval = time.time()
    requetes_traitees = 0
    
    # Logs de debug pour les premi√®res requ√™tes
    debug_logs_affiches = 0
    
    for i, (requete, ids_attendus, difficulte, type_req) in enumerate(requetes, 1):
        # Skip requ√™tes images non compl√©t√©es
        if "[REQUETE_A_COMPLETER]" in requete:
            continue
        
        # Encoder requ√™te
        if encodeur:
            emb_req = encodeur.encode([requete], normalize_embeddings=True, show_progress_bar=False)[0]
        else:
            emb_brut = encoder_texte_via_api([requete], modele_info['id'])[0]
            emb_req = np.array(emb_brut)
        
        # Rechercher
        resultats = db.rechercher(
            nom_collection=nom_collection,
            embedding_requete=emb_req.tolist(),
            nombre_resultats=10,
            methode="KNN",
        )
        
        # IDs r√©sultats
        ids_resultats = [r["metadata"]["id"] for r in resultats]
        
        # Debug pour les 3 premi√®res requ√™tes
        if debug_logs_affiches < 3:
            print(f"\n   üîç DEBUG Requ√™te #{i} [{type_req}] [{difficulte}]:")
            print(f"      Requ√™te: {requete[:60]}...")
            print(f"      IDs attendus: {ids_attendus[:3]}")
            print(f"      IDs obtenus: {ids_resultats[:5]}")
            # V√©rifier si au moins 1 ID attendu est dans les r√©sultats
            trouve = any(id_att in ids_resultats[:TOP_K_BENCHMARK] for id_att in ids_attendus)
            print(f"      ‚úÖ TROUV√â dans top {TOP_K_BENCHMARK}" if trouve else f"      ‚ùå PAS TROUV√â dans top {TOP_K_BENCHMARK}")
            debug_logs_affiches += 1
        
        # Calculer m√©triques
        p1 = precision_at_k(ids_resultats, ids_attendus, 1)
        p3 = precision_at_k(ids_resultats, ids_attendus, 3)
        pk = precision_at_k(ids_resultats, ids_attendus, TOP_K_BENCHMARK)
        rk = recall_at_k(ids_resultats, ids_attendus, TOP_K_BENCHMARK)
        mrr = mean_reciprocal_rank(ids_resultats, ids_attendus)
        reussite = 1.0 if est_recherche_reussie(ids_resultats, ids_attendus, TOP_K_BENCHMARK) else 0.0
        
        metriques["precision@1"].append(p1)
        metriques["precision@3"].append(p3)
        metriques[f"precision@{TOP_K_BENCHMARK}"].append(pk)
        metriques[f"recall@{TOP_K_BENCHMARK}"].append(rk)
        metriques["mrr"].append(mrr)
        metriques["reussites"].append(reussite)
        
        metriques_par_type[type_req][f"precision@{TOP_K_BENCHMARK}"].append(pk)
        metriques_par_type[type_req][f"recall@{TOP_K_BENCHMARK}"].append(rk)
        metriques_par_type[type_req]["mrr"].append(mrr)
        metriques_par_type[type_req]["reussites"].append(reussite)
        
        requetes_traitees += 1
        
        if i % 10 == 0:
            temps_ecoule = time.time() - debut_eval
            temps_par_requete = temps_ecoule / requetes_traitees
            requetes_restantes = len(requetes) - i
            temps_restant = temps_par_requete * requetes_restantes
            taux_reussite_actuel = np.mean(metriques["reussites"]) * 100
            
            print(f"   [{i}/{len(requetes)}] {(i/len(requetes)*100):.1f}% - "
                  f"Taux r√©ussite: {taux_reussite_actuel:.1f}% - "
                  f"√âcoul√©: {temps_ecoule:.1f}s - Restant: {temps_restant:.1f}s")
    
    # Calculer moyennes
    taux_reussite = np.mean(metriques["reussites"]) * 100  # Score sur 100
    
    print(f"\n   üìà Statistiques finales:")
    print(f"      Requ√™tes trait√©es: {requetes_traitees}")
    print(f"      Requ√™tes r√©ussies: {int(sum(metriques['reussites']))}/{requetes_traitees}")
    print(f"      Taux de r√©ussite: {taux_reussite:.1f}%")
    
    resultats = {
        "modele": modele_info['nom'],
        "precision@1": np.mean(metriques["precision@1"]),
        "precision@3": np.mean(metriques["precision@3"]),
        f"precision@{TOP_K_BENCHMARK}": np.mean(metriques[f"precision@{TOP_K_BENCHMARK}"]),
        f"recall@{TOP_K_BENCHMARK}": np.mean(metriques[f"recall@{TOP_K_BENCHMARK}"]),
        "mrr": np.mean(metriques["mrr"]),
        "taux_reussite": taux_reussite,  # Pourcentage de requ√™tes r√©ussies
        "par_type": {
            "message": {
                f"precision@{TOP_K_BENCHMARK}": np.mean(metriques_par_type["message"][f"precision@{TOP_K_BENCHMARK}"]) if metriques_par_type["message"][f"precision@{TOP_K_BENCHMARK}"] else 0,
                f"recall@{TOP_K_BENCHMARK}": np.mean(metriques_par_type["message"][f"recall@{TOP_K_BENCHMARK}"]) if metriques_par_type["message"][f"recall@{TOP_K_BENCHMARK}"] else 0,
                "mrr": np.mean(metriques_par_type["message"]["mrr"]) if metriques_par_type["message"]["mrr"] else 0,
                "taux_reussite": np.mean(metriques_par_type["message"]["reussites"]) * 100 if metriques_par_type["message"]["reussites"] else 0,
            },
            "image": {
                f"precision@{TOP_K_BENCHMARK}": np.mean(metriques_par_type["image"][f"precision@{TOP_K_BENCHMARK}"]) if metriques_par_type["image"][f"precision@{TOP_K_BENCHMARK}"] else 0,
                f"recall@{TOP_K_BENCHMARK}": np.mean(metriques_par_type["image"][f"recall@{TOP_K_BENCHMARK}"]) if metriques_par_type["image"][f"recall@{TOP_K_BENCHMARK}"] else 0,
                "mrr": np.mean(metriques_par_type["image"]["mrr"]) if metriques_par_type["image"]["mrr"] else 0,
                "taux_reussite": np.mean(metriques_par_type["image"]["reussites"]) * 100 if metriques_par_type["image"]["reussites"] else 0,
            },
        },
    }
    
    print(f"   ‚úÖ √âvaluation termin√©e")
    print(f"   üìä Score: {taux_reussite:.1f}% | P@{TOP_K_BENCHMARK}: {resultats[f'precision@{TOP_K_BENCHMARK}']:.3f} | MRR: {resultats['mrr']:.3f}")
    
    return resultats


# ============================================================================
# G√âN√âRATION DU RAPPORT MARKDOWN
# ============================================================================

def generer_rapport_markdown(durees_texte: Dict, durees_images: Dict, resultats_benchmark: List[Dict]):
    """G√©n√®re le rapport markdown complet.
    
    Args:
        durees_texte: Dur√©es d'encodage texte
        durees_images: Dur√©es d'encodage images
        resultats_benchmark: R√©sultats du benchmark de qualit√©
    """
    print(f"\nüìù G√©n√©ration du rapport markdown...")
    print(f"   Fichier: {CHEMIN_OUTPUT_MD}")
    
    contenu = f"""# Benchmark OPSEMIA - R√©sultats Complets

**Date:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}  
**Dataset:** Cas4 Breaking Bad (560 messages + 10 images)  
**Requ√™tes de test:** 100 (90 messages + 10 images)  
**Crit√®re de r√©ussite:** Au moins 1 r√©sultat pertinent dans le top {TOP_K_BENCHMARK}

---

## üìä R√©sum√© Ex√©cutif

Ce benchmark √©value les performances de 4 mod√®les d'embedding texte et du mod√®le BLIP pour la description d'images, √† travers :
1. **Calcul de dur√©es d'encodage** : temps n√©cessaire pour encoder des messages et images
2. **Benchmark de qualit√©** : pr√©cision de la recherche s√©mantique sur 100 requ√™tes r√©elles

### Crit√®re de R√©ussite

Une recherche est consid√©r√©e comme **r√©ussie** si au moins 1 document pertinent appara√Æt dans les **{TOP_K_BENCHMARK} premiers r√©sultats**.
Le **taux de r√©ussite** (score sur 100) repr√©sente le pourcentage de requ√™tes r√©ussies.

### Mod√®les Test√©s

| Mod√®le | Param√®tres | Dimensions | Type |
|--------|-----------|------------|------|
| **Jina-v3** | 137M | 1024 | Local |
| **Solon-large** | 335M | 1024 | Local |
| **BGE-M3** | 568M | 1024 | Local |
| **Qwen3-8B** | 8000M | 8192 | API (DeepInfra) |
| **BLIP** | - | - | Vision (description) |

---

## ‚è±Ô∏è Partie 1 : Dur√©es d'Encodage

### 1.1 Mod√®les Texte

"""
    
    # Tableau des dur√©es texte
    contenu += "| Mod√®le | Chargement | Msg Court | 100 Tokens | 1000 Msgs | D√©bit | Est. 1M Msgs |\n"
    contenu += "|--------|-----------|-----------|------------|-----------|-------|-------------|\n"
    
    # S√©parer mod√®les valides et en erreur
    durees_valides = {nom: d for nom, d in durees_texte.items() if 'erreur' not in d}
    durees_erreurs = {nom: d for nom, d in durees_texte.items() if 'erreur' in d}
    
    for modele_nom, durees in durees_valides.items():
        chargement = durees.get('duree_chargement_s', 'N/A')
        if isinstance(chargement, (int, float)):
            chargement = f"{chargement:.1f}s"
        
        msg_court = durees.get('duree_message_court_ms', 'N/A')
        if isinstance(msg_court, (int, float)):
            msg_court = f"{msg_court:.1f}ms"
        
        tokens_100 = durees.get('duree_100_tokens_ms', 'N/A')
        if isinstance(tokens_100, (int, float)):
            tokens_100 = f"{tokens_100:.1f}ms"
        
        msgs_1000 = durees.get('duree_1000_messages_s', 'N/A')
        if isinstance(msgs_1000, (int, float)):
            msgs_1000 = f"{msgs_1000:.1f}s"
        
        debit = durees.get('debit_msg_par_s', 'N/A')
        if isinstance(debit, (int, float)):
            debit = f"{debit:.1f}/s"
        
        est_100k = durees.get('estimation_100k_messages_h', 'N/A')
        if isinstance(est_100k, (int, float)):
            est_100k = f"{est_100k:.1f}h"
        
        contenu += f"| **{modele_nom}** | {chargement} | {msg_court} | {tokens_100} | {msgs_1000} | {debit} | {est_100k} |\n"
        
        # Note pour Qwen3
        if 'note' in durees:
            contenu += f"\n> üìù **Note {modele_nom}:** {durees['note']}\n\n"
    
    # Afficher les erreurs si pr√©sentes
    if durees_erreurs:
        contenu += "\n#### ‚ö†Ô∏è Mod√®les en erreur\n\n"
        for modele_nom, durees in durees_erreurs.items():
            contenu += f"- **{modele_nom}**: {durees['erreur']}\n"
        contenu += "\n"
    
    contenu += "\n### 1.2 Images (BLIP + Encodage Description)\n\n"
    
    if durees_images:
        contenu += f"""
| M√©trique | Valeur |
|----------|--------|
| **Chargement BLIP** | {durees_images['duree_chargement_s']:.1f}s |
| **Description moyenne** | {durees_images['duree_description_moyenne_s']:.2f}s |
| **Encodage description** | {durees_images['duree_encodage_desc_moyenne_ms']:.1f}ms |
| **Total moyen/image** | {durees_images['duree_totale_moyenne_s']:.2f}s |
| **Plus rapide** | {durees_images['duree_min_s']:.2f}s |
| **Plus lent** | {durees_images['duree_max_s']:.2f}s |
| **Estimation 1000 images** | {durees_images['estimation_1000_images_h']:.1f}h |

"""
    
    contenu += """
### 1.3 Estimations Globales

#### Pour 100k Messages

"""
    
    for modele_nom, durees in durees_texte.items():
        est = durees.get('estimation_100k_messages_h', 'N/A')
        if isinstance(est, str) and est.startswith('~'):
            contenu += f"- **{modele_nom}:** {est}\n"
        elif isinstance(est, (int, float)):
            contenu += f"- **{modele_nom}:** {est:.1f} heures\n"
    
    if durees_images:
        contenu += f"\n#### Pour 1000 Images\n\n"
        contenu += f"- **BLIP + Encodage:** {durees_images['estimation_1000_images_h']:.1f} heures\n"
    
    if resultats_benchmark:
        contenu += "\n---\n\n## üéØ Partie 2 : Benchmark de Qualit√© (Pr√©cision)\n\n"
        contenu += "### 2.1 R√©sultats Globaux (100 requ√™tes)\n\n"
        
        # Tableau des r√©sultats
        contenu += f"| Mod√®le | **Score (/100)** | P@1 | P@3 | P@{TOP_K_BENCHMARK} | R@{TOP_K_BENCHMARK} | MRR |\n"
        contenu += "|--------|------------------|-----|-----|-----|-----|-----|\n"
        
        # S√©parer r√©sultats valides et erreurs
        resultats_valides = [r for r in resultats_benchmark if 'erreur' not in r]
        resultats_erreurs = [r for r in resultats_benchmark if 'erreur' in r]
        
        # Trier par taux de r√©ussite (score sur 100)
        resultats_tries = sorted(resultats_valides, key=lambda x: x.get('taux_reussite', 0), reverse=True)
        
        for res in resultats_tries:
            score = res.get('taux_reussite', 0)
            contenu += f"| **{res['modele']}** | **{score:.1f}%** | {res['precision@1']:.3f} | {res['precision@3']:.3f} | {res.get(f'precision@{TOP_K_BENCHMARK}', 0):.3f} | {res.get(f'recall@{TOP_K_BENCHMARK}', 0):.3f} | {res['mrr']:.3f} |\n"
        
        # Afficher les erreurs si pr√©sentes
        if resultats_erreurs:
            contenu += "\n#### ‚ö†Ô∏è Mod√®les en erreur\n\n"
            for res in resultats_erreurs:
                contenu += f"- **{res['modele']}**: {res['erreur']}\n"
            contenu += "\n"
    else:
        contenu += "\n---\n\n## üéØ Partie 2 : Benchmark de Qualit√© (Pr√©cision)\n\n"
        contenu += "**‚è≠Ô∏è Tests de qualit√© non ex√©cut√©s**\n\n"
        resultats_tries = []
    
    contenu += "\n### 2.2 R√©sultats par Type de Contenu\n\n"
    
    # Messages
    contenu += "#### Messages (90 requ√™tes)\n\n"
    contenu += f"| Mod√®le | Taux R√©ussite | P@{TOP_K_BENCHMARK} | R@{TOP_K_BENCHMARK} | MRR |\n"
    contenu += "|--------|---------------|-----|-----|-----|\n"
    
    for res in resultats_tries:
        if 'par_type' in res and 'message' in res['par_type']:
            msg_res = res['par_type']['message']
            contenu += f"| **{res['modele']}** | {msg_res.get('taux_reussite', 0):.1f}% | {msg_res.get(f'precision@{TOP_K_BENCHMARK}', 0):.3f} | {msg_res.get(f'recall@{TOP_K_BENCHMARK}', 0):.3f} | {msg_res['mrr']:.3f} |\n"
    
    # Images
    contenu += "\n#### Images (10 requ√™tes)\n\n"
    contenu += f"| Mod√®le | Taux R√©ussite | P@{TOP_K_BENCHMARK} | R@{TOP_K_BENCHMARK} | MRR |\n"
    contenu += "|--------|---------------|-----|-----|-----|\n"
    
    for res in resultats_tries:
        if 'par_type' in res and 'image' in res['par_type']:
            img_res = res['par_type']['image']
            contenu += f"| **{res['modele']}** | {img_res.get('taux_reussite', 0):.1f}% | {img_res.get(f'precision@{TOP_K_BENCHMARK}', 0):.3f} | {img_res.get(f'recall@{TOP_K_BENCHMARK}', 0):.3f} | {img_res['mrr']:.3f} |\n"
    
    contenu += "\n---\n\n## üèÜ Conclusions\n\n"
    
    # Meilleur mod√®le (si r√©sultats disponibles)
    if resultats_tries:
        meilleur = resultats_tries[0]
        contenu += f"### Meilleur Mod√®le Global: **{meilleur['modele']}**\n\n"
        
        # V√©rifier si le mod√®le a une erreur
        if 'erreur' in meilleur:
            contenu += f"‚ö†Ô∏è **Erreur:** {meilleur['erreur']}\n\n"
        else:
            contenu += f"- **Score:** {meilleur.get('taux_reussite', 0):.1f}/100 ({meilleur.get('taux_reussite', 0):.1f}% de requ√™tes r√©ussies)\n"
            contenu += f"- **MRR:** {meilleur['mrr']:.3f}\n"
            contenu += f"- **Precision@{TOP_K_BENCHMARK}:** {meilleur.get(f'precision@{TOP_K_BENCHMARK}', 0):.3f}\n"
            contenu += f"- **Recall@{TOP_K_BENCHMARK}:** {meilleur.get(f'recall@{TOP_K_BENCHMARK}', 0):.3f}\n\n"
    else:
        contenu += "‚ö†Ô∏è **Aucun r√©sultat disponible** (tous les mod√®les ont √©chou√© ou tests ignor√©s)\n\n"
    
    # Compromis vitesse/qualit√©
    contenu += "### Compromis Vitesse/Qualit√©\n\n"
    
    for res in resultats_tries:
        duree_modele = durees_texte.get(res['modele'], {})
        est_100k = duree_modele.get('estimation_100k_messages_h', 'N/A')
        
        if isinstance(est_100k, str):
            est_str = est_100k
        elif isinstance(est_100k, (int, float)):
            est_str = f"{est_100k:.1f}h"
        else:
            est_str = "N/A"
        
        score = res.get('taux_reussite', 0)
        contenu += f"- **{res['modele']}:** Score {score:.1f}/100, Vitesse {est_str} pour 100k msgs\n"
    
    contenu += "\n### Recommandations\n\n"
    contenu += f"1. **Pour la qualit√© maximale:** Privil√©gier le mod√®le avec le meilleur score (taux de r√©ussite sur {TOP_K_BENCHMARK} r√©sultats)\n"
    contenu += "2. **Pour la vitesse:** Choisir un mod√®le avec moins de param√®tres (Jina-v3 ou Solon-large)\n"
    contenu += "3. **Pour les images:** BLIP fournit des descriptions de qualit√© mais reste lent (~2s/image)\n"
    contenu += "4. **Pour le d√©ploiement:** Consid√©rer le compromis entre score de qualit√© et temps d'encodage\n\n"
    contenu += f"**Note:** Le crit√®re de r√©ussite (top {TOP_K_BENCHMARK}) est configurable dans `config/settings.py` via `TOP_K_BENCHMARK`.\n\n"
    
    contenu += "---\n\n"
    contenu += f"*Rapport g√©n√©r√© automatiquement par `benchmark_complet_opsemia.py` le {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')}*\n"
    
    # √âcrire le fichier
    CHEMIN_OUTPUT_MD.parent.mkdir(parents=True, exist_ok=True)
    with open(CHEMIN_OUTPUT_MD, 'w', encoding='utf-8') as f:
        f.write(contenu)
    
    print(f"   ‚úÖ Rapport g√©n√©r√©: {CHEMIN_OUTPUT_MD}")


# ============================================================================
# MAIN
# ============================================================================

def main():
    """Fonction principale du benchmark complet."""
    # Parser les arguments
    parser = argparse.ArgumentParser(description="Benchmark complet OPSEMIA : Rapidit√© vs Qualit√©")
    parser.add_argument(
        "--force", 
        action="store_true", 
        help="Force la r√©indexation m√™me si les collections existent d√©j√†"
    )
    parser.add_argument(
        "--skip-speed", 
        action="store_true", 
        help="Sauter les tests de rapidit√© d'encodage (calcul de dur√©es)"
    )
    parser.add_argument(
        "--skip-quality", 
        action="store_true", 
        help="Sauter les tests de qualit√© (benchmark 100 requ√™tes)"
    )
    args = parser.parse_args()
    
    print("="*80)
    print("BENCHMARK COMPLET OPSEMIA : RAPIDIT√â vs QUALIT√â")
    print("="*80)
    print(f"Dataset: {CHEMIN_CSV_MESSAGES.name}")
    print(f"Images: {len(list(DOSSIER_IMAGES.glob('*')))} fichiers")
    print(f"Requ√™tes: 100 (90 messages + 10 images)")
    if args.force:
        print("‚ö†Ô∏è  Mode FORCE : R√©indexation compl√®te")
    if args.skip_speed:
        print("‚è≠Ô∏è  Tests de rapidit√© d√©sactiv√©s")
    if args.skip_quality:
        print("‚è≠Ô∏è  Tests de qualit√© d√©sactiv√©s")
    print("="*80)
    
    # ========== PARTIE 1: CALCUL DES DUR√âES ==========
    if not args.skip_speed:
        print("\nüöÄ PARTIE 1/2 : CALCUL DES DUR√âES D'ENCODAGE (RAPIDIT√â)\n")
        
        durees_texte = calculer_durees_encodage_texte()
        durees_images = calculer_durees_encodage_images()
    else:
        print("\n‚è≠Ô∏è  PARTIE 1/2 : CALCUL DES DUR√âES IGNOR√â\n")
        durees_texte = {}
        durees_images = {}
    
    # ========== PARTIE 2: BENCHMARK DE QUALIT√â ==========
    if not args.skip_quality:
        print("\nüöÄ PARTIE 2/2 : BENCHMARK DE QUALIT√â (PR√âCISION)\n")
    else:
        print("\n‚è≠Ô∏è  PARTIE 2/2 : BENCHMARK DE QUALIT√â IGNOR√â\n")
    
    resultats_benchmark = []
    
    # Charger parametres
    parametres = obtenir_parametres()
    
    # Cr√©er encodeur d'images (partag√© pour tous les mod√®les)
    encodeur_texte_temp = EncodeurTexte(
        id_modele=parametres.ID_MODELE_EMBEDDING,
        preference_peripherique=parametres.PERIPHERIQUE_EMBEDDING
    )
    encodeur_image = EncodeurImage(
        encodeur_texte=encodeur_texte_temp,
        preference_peripherique=parametres.PERIPHERIQUE_EMBEDDING,
    )
    
    for modele_info in MODELES_TEXTE:
        print(f"\n{'='*80}")
        print(f"MOD√àLE: {modele_info['nom']}")
        print(f"{'='*80}")
        
        try:
            # Charger encodeur
            if modele_info['local']:
                try:
                    encodeur = SentenceTransformer(modele_info['id'], trust_remote_code=True)
                except:
                    encodeur = SentenceTransformer(modele_info['id'])
                
                encodeur_wrap = EncodeurTexte(
                    id_modele=modele_info['id'],
                    preference_peripherique=parametres.PERIPHERIQUE_EMBEDDING
                )
            else:
                encodeur = None
                encodeur_wrap = None
            
            if not args.skip_quality:
                # Indexer dataset
                nom_collection = indexer_dataset_benchmark(
                    modele_info, 
                    encodeur_wrap, 
                    encodeur_image,
                    force_reindex=args.force
                )
                
                # √âvaluer
                resultats = evaluer_modele_benchmark(modele_info, nom_collection)
                resultats_benchmark.append(resultats)
                
                # Nettoyer uniquement si force (sinon garder le cache)
                if args.force:
                    db = BaseVectorielle(chemin_persistance=CHEMIN_DB_BENCHMARK)
                    db.supprimer_collection(nom_collection)
                    print(f"   üóëÔ∏è  Collection temporaire supprim√©e")
                else:
                    print(f"   üíæ Collection conserv√©e pour cache")
                    
        except Exception as e:
            print(f"\n‚ùå ERREUR CRITIQUE avec le mod√®le {modele_info['nom']}: {type(e).__name__}: {str(e)}")
            print(f"   ‚ö†Ô∏è  Le mod√®le sera ignor√© dans les r√©sultats")
            print(f"   ‚è© Passage au mod√®le suivant...\n")
            
            # Ajouter un r√©sultat par d√©faut avec erreur
            if not args.skip_quality:
                resultats_benchmark.append({
                    "modele": modele_info['nom'],
                    "erreur": str(e),
                    "precision@1": 0,
                    "precision@3": 0,
                    f"precision@{TOP_K_BENCHMARK}": 0,
                    f"recall@{TOP_K_BENCHMARK}": 0,
                    "mrr": 0,
                    "taux_reussite": 0,
                })
        
        finally:
            # Lib√©rer la m√©moire
            if 'encodeur' in locals():
                del encodeur
            if 'encodeur_wrap' in locals():
                del encodeur_wrap
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            import gc
            gc.collect()
    
    # ========== PARTIE 3: G√âN√âRATION RAPPORT ==========
    if not args.skip_speed or not args.skip_quality:
        print("\nüöÄ PARTIE 3/3 : G√âN√âRATION DU RAPPORT\n")
        
        generer_rapport_markdown(durees_texte, durees_images, resultats_benchmark)
        
        print("\n" + "="*80)
        print("‚úÖ BENCHMARK TERMIN√â AVEC SUCC√àS!")
        print("="*80)
        print(f"üìÑ Rapport disponible: {CHEMIN_OUTPUT_MD}")
        if not args.skip_quality:
            print(f"üìä {len(resultats_benchmark)} mod√®les test√©s")
            print(f"üéØ {sum([len(obtenir_requetes_messages()), len(obtenir_requetes_images())])} requ√™tes √©valu√©es")
        if not args.force and not args.skip_quality:
            print(f"üíæ Collections conserv√©es dans {CHEMIN_DB_BENCHMARK}")
            print(f"   Utilisez scripts/nettoyer_benchmark.py pour nettoyer")
        print("="*80)
    else:
        print("\n" + "="*80)
        print("‚ö†Ô∏è  Aucun test ex√©cut√© (tous ignor√©s)")
        print("="*80)


if __name__ == "__main__":
    main()

