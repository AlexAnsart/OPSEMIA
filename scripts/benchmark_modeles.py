#!/usr/bin/env python3
"""Script de benchmark pour comparer les performances des mod√®les d'embedding.

Ce script √©value plusieurs mod√®les d'embedding sur un dataset de test th√©matique
et calcule des m√©triques de performance:
- Precision@K : Proportion de documents pertinents parmi les K premiers r√©sultats
- Recall@K : Proportion de documents pertinents retrouv√©s parmi tous les pertinents
- MRR (Mean Reciprocal Rank) : Moyenne des rangs r√©ciproques du premier r√©sultat pertinent
- NDCG@K : Normalized Discounted Cumulative Gain (qualit√© du classement)
- Temps d'encodage et de recherche
"""

import sys
import time
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
from sentence_transformers import SentenceTransformer

# Ajouter le r√©pertoire racine au path pour les imports
racine_projet = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(racine_projet))

from scripts.donnees_benchmark import obtenir_documents_test, obtenir_requetes_test


# Mod√®les √† tester
MODELES_A_TESTER = [
    ("BAAI/bge-m3", "BGE-M3 (baseline)"),
    ("jinaai/jina-embeddings-v3", "Jina-v3"),
    ("Qwen/Qwen3-Embedding-8B", "Qwen3-8B"),
    # Le mod√®le Solon sera ajout√© si l'ID est confirm√©
]


def calculer_similarite_cosine(vecteur1: np.ndarray, vecteur2: np.ndarray) -> float:
    """Calcule la similarit√© cosine entre deux vecteurs (d√©j√† normalis√©s)."""
    return float(np.dot(vecteur1, vecteur2))


def precision_at_k(resultats_ids: List[str], pertinents_ids: List[str], k: int) -> float:
    """Calcule la Precision@K.
    
    Args:
        resultats_ids: Liste des IDs retourn√©s (ordonn√©s par pertinence)
        pertinents_ids: Liste des IDs pertinents attendus (ground truth)
        k: Nombre de premiers r√©sultats √† consid√©rer
    
    Returns:
        Score de precision entre 0 et 1
    """
    if k == 0:
        return 0.0
    
    top_k = resultats_ids[:k]
    nb_pertinents_trouves = len([doc_id for doc_id in top_k if doc_id in pertinents_ids])
    return nb_pertinents_trouves / k


def recall_at_k(resultats_ids: List[str], pertinents_ids: List[str], k: int) -> float:
    """Calcule le Recall@K.
    
    Args:
        resultats_ids: Liste des IDs retourn√©s
        pertinents_ids: Liste des IDs pertinents attendus
        k: Nombre de premiers r√©sultats √† consid√©rer
    
    Returns:
        Score de recall entre 0 et 1
    """
    if len(pertinents_ids) == 0:
        return 0.0
    
    top_k = resultats_ids[:k]
    nb_pertinents_trouves = len([doc_id for doc_id in top_k if doc_id in pertinents_ids])
    return nb_pertinents_trouves / len(pertinents_ids)


def mean_reciprocal_rank(resultats_ids: List[str], pertinents_ids: List[str]) -> float:
    """Calcule le Mean Reciprocal Rank (MRR).
    
    Le MRR mesure √† quelle position appara√Æt le premier r√©sultat pertinent.
    MRR = 1/rang du premier r√©sultat pertinent
    
    Returns:
        Score MRR entre 0 et 1 (1 = meilleur r√©sultat en position 1)
    """
    for i, doc_id in enumerate(resultats_ids, 1):
        if doc_id in pertinents_ids:
            return 1.0 / i
    return 0.0


def ndcg_at_k(resultats_ids: List[str], pertinents_ids: List[str], k: int) -> float:
    """Calcule le Normalized Discounted Cumulative Gain (NDCG@K).
    
    Le NDCG mesure la qualit√© du classement en donnant plus de poids aux
    r√©sultats pertinents qui apparaissent t√¥t dans la liste.
    
    Returns:
        Score NDCG entre 0 et 1 (1 = classement parfait)
    """
    def dcg(relevances: List[int], k: int) -> float:
        """Calcule le DCG."""
        dcg_score = 0.0
        for i, rel in enumerate(relevances[:k], 1):
            dcg_score += rel / np.log2(i + 1)
        return dcg_score
    
    # Relevances pour les r√©sultats obtenus
    relevances = [1 if doc_id in pertinents_ids else 0 for doc_id in resultats_ids]
    
    # DCG id√©al (r√©sultats pertinents d'abord)
    ideal_relevances = [1] * min(len(pertinents_ids), k) + [0] * (k - len(pertinents_ids))
    
    dcg_score = dcg(relevances, k)
    idcg_score = dcg(ideal_relevances, k)
    
    if idcg_score == 0:
        return 0.0
    
    return dcg_score / idcg_score


def evaluer_modele(
    id_modele: str,
    nom_modele: str,
    documents: List[Dict],
    requetes: List[Tuple[str, List[str], str]],
    k: int = 5,
) -> Dict:
    """√âvalue un mod√®le sur le dataset de test.
    
    Args:
        id_modele: ID Hugging Face du mod√®le
        nom_modele: Nom lisible du mod√®le
        documents: Liste des documents de test
        requetes: Liste des requ√™tes avec ground truth
        k: Nombre de r√©sultats √† consid√©rer pour les m√©triques @K
    
    Returns:
        Dictionnaire avec les r√©sultats et m√©triques
    """
    print(f"\n{'='*70}")
    print(f"√âVALUATION: {nom_modele}")
    print(f"{'='*70}")
    print(f"ID Mod√®le: {id_modele}")
    
    try:
        # Charger le mod√®le
        print("‚è≥ Chargement du mod√®le...")
        debut_chargement = time.time()
        
        # trust_remote_code=True pour certains mod√®les comme Jina
        try:
            modele = SentenceTransformer(id_modele, trust_remote_code=True)
        except Exception:
            modele = SentenceTransformer(id_modele)
        
        temps_chargement = time.time() - debut_chargement
        print(f"‚úÖ Mod√®le charg√© ({temps_chargement:.2f}s)")
        print(f"   Dimensions: {modele.get_sentence_embedding_dimension()}")
        
        # Encoder les documents
        print("\n‚è≥ Encodage des documents...")
        debut_encodage = time.time()
        textes_docs = [doc["texte"] for doc in documents]
        embeddings_docs = modele.encode(
            textes_docs,
            normalize_embeddings=True,
            show_progress_bar=False,
        )
        temps_encodage_docs = time.time() - debut_encodage
        print(f"‚úÖ {len(documents)} documents encod√©s ({temps_encodage_docs:.2f}s)")
        
        # √âvaluer sur chaque requ√™te
        print(f"\n‚è≥ √âvaluation sur {len(requetes)} requ√™tes...")
        
        metriques_globales = {
            "precision@3": [],
            "recall@3": [],
            "precision@5": [],
            "recall@5": [],
            "mrr": [],
            "ndcg@5": [],
            "temps_recherche": [],
        }
        
        # M√©triques par difficult√©
        metriques_par_difficulte = {
            "facile": {"precision@5": [], "recall@5": [], "mrr": []},
            "moyen": {"precision@5": [], "recall@5": [], "mrr": []},
            "difficile": {"precision@5": [], "recall@5": [], "mrr": []},
        }
        
        for i, (requete, docs_pertinents, difficulte) in enumerate(requetes, 1):
            # Encoder la requ√™te
            debut_recherche = time.time()
            embedding_requete = modele.encode(
                [requete],
                normalize_embeddings=True,
                show_progress_bar=False,
            )[0]
            
            # Calculer les similarit√©s
            similarites = [
                (doc["id"], calculer_similarite_cosine(embedding_requete, emb_doc))
                for doc, emb_doc in zip(documents, embeddings_docs)
            ]
            
            # Trier par similarit√© d√©croissante
            similarites.sort(key=lambda x: x[1], reverse=True)
            resultats_ids = [doc_id for doc_id, _ in similarites]
            
            temps_recherche = time.time() - debut_recherche
            
            # Calculer les m√©triques
            p3 = precision_at_k(resultats_ids, docs_pertinents, 3)
            r3 = recall_at_k(resultats_ids, docs_pertinents, 3)
            p5 = precision_at_k(resultats_ids, docs_pertinents, 5)
            r5 = recall_at_k(resultats_ids, docs_pertinents, 5)
            mrr_score = mean_reciprocal_rank(resultats_ids, docs_pertinents)
            ndcg = ndcg_at_k(resultats_ids, docs_pertinents, 5)
            
            # Stocker les m√©triques
            metriques_globales["precision@3"].append(p3)
            metriques_globales["recall@3"].append(r3)
            metriques_globales["precision@5"].append(p5)
            metriques_globales["recall@5"].append(r5)
            metriques_globales["mrr"].append(mrr_score)
            metriques_globales["ndcg@5"].append(ndcg)
            metriques_globales["temps_recherche"].append(temps_recherche)
            
            # Stocker par difficult√©
            metriques_par_difficulte[difficulte]["precision@5"].append(p5)
            metriques_par_difficulte[difficulte]["recall@5"].append(r5)
            metriques_par_difficulte[difficulte]["mrr"].append(mrr_score)
            
            # Afficher progression
            if i % 5 == 0 or i == len(requetes):
                print(f"   Requ√™te {i}/{len(requetes)} trait√©e...")
        
        # Calculer les moyennes
        resultats = {
            "id_modele": id_modele,
            "nom_modele": nom_modele,
            "dimensions": modele.get_sentence_embedding_dimension(),
            "temps_chargement_sec": temps_chargement,
            "temps_encodage_docs_sec": temps_encodage_docs,
            "temps_moyen_recherche_ms": np.mean(metriques_globales["temps_recherche"]) * 1000,
            "precision@3": np.mean(metriques_globales["precision@3"]),
            "recall@3": np.mean(metriques_globales["recall@3"]),
            "precision@5": np.mean(metriques_globales["precision@5"]),
            "recall@5": np.mean(metriques_globales["recall@5"]),
            "mrr": np.mean(metriques_globales["mrr"]),
            "ndcg@5": np.mean(metriques_globales["ndcg@5"]),
            "metriques_par_difficulte": {
                diff: {
                    "precision@5": np.mean(metriques["precision@5"]) if metriques["precision@5"] else 0,
                    "recall@5": np.mean(metriques["recall@5"]) if metriques["recall@5"] else 0,
                    "mrr": np.mean(metriques["mrr"]) if metriques["mrr"] else 0,
                }
                for diff, metriques in metriques_par_difficulte.items()
            },
            "succes": True,
        }
        
        print(f"\n‚úÖ √âvaluation termin√©e!")
        afficher_resultats_modele(resultats)
        
        return resultats
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de l'√©valuation: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            "id_modele": id_modele,
            "nom_modele": nom_modele,
            "succes": False,
            "erreur": str(e),
        }


def afficher_resultats_modele(resultats: Dict):
    """Affiche les r√©sultats d'√©valuation d'un mod√®le."""
    if not resultats.get("succes", False):
        print(f"‚ùå √âchec: {resultats.get('erreur', 'Erreur inconnue')}")
        return
    
    print(f"\nüìä R√âSULTATS:")
    print(f"   Dimensions: {resultats['dimensions']}")
    print(f"\n‚è±Ô∏è  PERFORMANCE:")
    print(f"   Chargement: {resultats['temps_chargement_sec']:.2f}s")
    print(f"   Encodage docs: {resultats['temps_encodage_docs_sec']:.2f}s")
    print(f"   Recherche moyenne: {resultats['temps_moyen_recherche_ms']:.2f}ms")
    print(f"\nüéØ QUALIT√â (moyenne sur toutes les requ√™tes):")
    print(f"   Precision@3: {resultats['precision@3']:.3f}")
    print(f"   Recall@3: {resultats['recall@3']:.3f}")
    print(f"   Precision@5: {resultats['precision@5']:.3f}")
    print(f"   Recall@5: {resultats['recall@5']:.3f}")
    print(f"   MRR: {resultats['mrr']:.3f}")
    print(f"   NDCG@5: {resultats['ndcg@5']:.3f}")
    
    print(f"\nüìà QUALIT√â PAR DIFFICULT√â:")
    for difficulte, metriques in resultats["metriques_par_difficulte"].items():
        print(f"   {difficulte.capitalize()}:")
        print(f"      P@5: {metriques['precision@5']:.3f} | R@5: {metriques['recall@5']:.3f} | MRR: {metriques['mrr']:.3f}")


def comparer_modeles(resultats_liste: List[Dict]):
    """Affiche un tableau comparatif des mod√®les."""
    print(f"\n{'='*100}")
    print("COMPARAISON DES MOD√àLES")
    print(f"{'='*100}")
    
    # Filtrer les mod√®les qui ont r√©ussi
    resultats_valides = [r for r in resultats_liste if r.get("succes", False)]
    
    if not resultats_valides:
        print("‚ùå Aucun mod√®le n'a pu √™tre √©valu√©.")
        return
    
    # En-t√™te du tableau
    print(f"\n{'Mod√®le':<25} | {'Dim':<6} | {'P@5':<6} | {'R@5':<6} | {'MRR':<6} | {'NDCG@5':<7} | {'Temps(ms)':<10}")
    print("-" * 100)
    
    # Trier par NDCG@5 (m√©trique globale de qualit√©)
    resultats_valides.sort(key=lambda x: x["ndcg@5"], reverse=True)
    
    for res in resultats_valides:
        nom = res["nom_modele"][:24]
        dims = res["dimensions"]
        p5 = res["precision@5"]
        r5 = res["recall@5"]
        mrr = res["mrr"]
        ndcg = res["ndcg@5"]
        temps = res["temps_moyen_recherche_ms"]
        
        print(f"{nom:<25} | {dims:<6} | {p5:.3f}  | {r5:.3f}  | {mrr:.3f}  | {ndcg:.3f}   | {temps:>8.2f}")
    
    print("-" * 100)
    
    # Afficher le gagnant
    meilleur = resultats_valides[0]
    print(f"\nüèÜ MEILLEUR MOD√àLE: {meilleur['nom_modele']}")
    print(f"   NDCG@5: {meilleur['ndcg@5']:.3f} | MRR: {meilleur['mrr']:.3f} | Precision@5: {meilleur['precision@5']:.3f}")


def main():
    """Fonction principale du benchmark."""
    print("=" * 100)
    print("BENCHMARK DES MOD√àLES D'EMBEDDING POUR OPSEMIA")
    print("=" * 100)
    
    # Charger les donn√©es de test
    documents = obtenir_documents_test()
    requetes = obtenir_requetes_test()
    
    print(f"\nüìä Dataset de test:")
    print(f"   - {len(documents)} documents")
    print(f"   - {len(requetes)} requ√™tes")
    
    # √âvaluer chaque mod√®le
    resultats_liste = []
    
    for id_modele, nom_modele in MODELES_A_TESTER:
        try:
            resultats = evaluer_modele(id_modele, nom_modele, documents, requetes, k=5)
            resultats_liste.append(resultats)
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  Interruption utilisateur. Arr√™t du benchmark.")
            break
        except Exception as e:
            print(f"\n‚ùå Erreur inattendue pour {nom_modele}: {e}")
            resultats_liste.append({
                "id_modele": id_modele,
                "nom_modele": nom_modele,
                "succes": False,
                "erreur": str(e),
            })
    
    # Comparer les r√©sultats
    if resultats_liste:
        comparer_modeles(resultats_liste)
    
    print(f"\n{'='*100}")
    print("‚úÖ BENCHMARK TERMIN√â")
    print(f"{'='*100}")


if __name__ == "__main__":
    main()

