# Index - Syst√®me de Benchmark OPSEMIA

Ce fichier r√©f√©rence tous les scripts et documentations du syst√®me de benchmark.

## üìö Documentation

### üöÄ D√©marrage Rapide
**[GUIDE_DEMARRAGE_BENCHMARK.md](GUIDE_DEMARRAGE_BENCHMARK.md)**
- Checklist avant de commencer
- Installation et configuration
- Ex√©cution en 3 √©tapes
- R√©solution de probl√®mes courants
- **üëâ Commencez par ici si c'est votre premi√®re fois**

### üìñ Documentation Compl√®te
**[README_BENCHMARK.md](README_BENCHMARK.md)**
- Description d√©taill√©e de tous les fichiers
- Explications des m√©triques
- Configuration avanc√©e
- Exemples de r√©sultats
- R√©f√©rences techniques

---

## üîß Scripts Principaux

### ‚≠ê Script Principal
**[benchmark_complet_opsemia.py](benchmark_complet_opsemia.py)**
- Ex√©cute le benchmark complet (dur√©es + qualit√©)
- Teste 4 mod√®les sur 100 requ√™tes
- G√©n√®re le rapport markdown final
- **Dur√©e:** 40-70 minutes

**Usage:**
```bash
python benchmark_complet_opsemia.py
```

---

## üìä Dataset et Configuration

### Dataset de Benchmark
**[donnees_benchmark_opsemia.py](donnees_benchmark_opsemia.py)**
- 90 requ√™tes pour messages (8 cat√©gories th√©matiques)
- 10 requ√™tes pour images
- R√©sultats attendus (ground truth)
- **‚ö†Ô∏è Compl√©ter les requ√™tes images avant l'ex√©cution**

### Configuration
Les mod√®les test√©s sont d√©finis dans `benchmark_complet_opsemia.py`:
- Jina-v3 (137M params, 1024 dims)
- Solon-large (335M params, 1024 dims)
- BGE-M3 (568M params, 1024 dims)
- Qwen3-8B (8000M params, 8192 dims, via API)

---

## üõ†Ô∏è Utilitaires

### Validation Pr√©-Benchmark
**[valider_benchmark.py](valider_benchmark.py)**
- V√©rifie tous les pr√©requis
- Teste les d√©pendances
- Valide la configuration
- Affiche les avertissements

**Usage:**
```bash
python valider_benchmark.py
```

**Quand l'utiliser:** Avant chaque ex√©cution du benchmark

---

### Visualisation du Dataset
**[afficher_dataset_benchmark.py](afficher_dataset_benchmark.py)**
- Affiche les statistiques du dataset
- Montre des exemples de requ√™tes
- Analyse la r√©partition par difficult√©
- Visualise la couverture du corpus

**Usage:**
```bash
python afficher_dataset_benchmark.py
```

**Quand l'utiliser:** Pour comprendre le dataset sans ex√©cuter le benchmark

---

### Test Rapide d'un Mod√®le
**[test_rapide_modele.py](test_rapide_modele.py)**
- Teste un seul mod√®le rapidement
- Utile pour le d√©veloppement
- V√©rifie que le mod√®le se charge
- Teste encodage et recherche

**Usage:**
```bash
python test_rapide_modele.py jina    # Tester Jina-v3
python test_rapide_modele.py bge     # Tester BGE-M3
python test_rapide_modele.py solon   # Tester Solon-large
```

**Quand l'utiliser:** Pour d√©boguer un mod√®le sp√©cifique

---

## üì• Scripts de T√©l√©chargement

Ces scripts t√©l√©chargent les mod√®les en avance pour √©viter les t√©l√©chargements pendant le benchmark:

- **[telecharger_modele_jina.py](telecharger_modele_jina.py)** - Jina-v3 (~2.5 GB)
- **[telecharger_modele_bge.py](telecharger_modele_bge.py)** - BGE-M3 (~2.2 GB)
- **[telecharger_modele_solon.py](telecharger_modele_solon.py)** - Solon-large (~1.3 GB)
- **[telecharger_modele_vision.py](telecharger_modele_vision.py)** - BLIP + Traduction (~2 GB)

**Usage:**
```bash
python telecharger_modele_jina.py
python telecharger_modele_bge.py
python telecharger_modele_solon.py
python telecharger_modele_vision.py
```

---

## üìÑ Fichiers G√©n√©r√©s

### Rapport de Benchmark
**`Docs Projet/BENCHMARK_RESULTATS_YYYYMMDD_HHMMSS.md`**

G√©n√©r√© automatiquement apr√®s ex√©cution du benchmark complet.

Contient:
- ‚úÖ Tableau comparatif des dur√©es d'encodage
- ‚úÖ Tableau comparatif de la qualit√© (m√©triques)
- ‚úÖ R√©sultats par type (messages vs images)
- ‚úÖ Analyse du compromis vitesse/qualit√©
- ‚úÖ Recommandations selon les besoins

### Collections Temporaires
**`data/benchmark_temp/`**

Collections ChromaDB cr√©√©es pendant le benchmark, automatiquement supprim√©es apr√®s.

---

## üîÑ Workflow Recommand√©

### Premi√®re Utilisation

1. **Lire la documentation**
   ```bash
   cat GUIDE_DEMARRAGE_BENCHMARK.md
   ```

2. **Valider le syst√®me**
   ```bash
   python valider_benchmark.py
   ```

3. **Compl√©ter les requ√™tes images**
   - √âditer `donnees_benchmark_opsemia.py`
   - Remplacer `[REQUETE_A_COMPLETER]` (10 requ√™tes)

4. **T√©l√©charger les mod√®les** (optionnel mais recommand√©)
   ```bash
   python telecharger_modele_jina.py
   python telecharger_modele_bge.py
   python telecharger_modele_solon.py
   python telecharger_modele_vision.py
   ```

5. **Visualiser le dataset** (optionnel)
   ```bash
   python afficher_dataset_benchmark.py
   ```

6. **Lancer le benchmark**
   ```bash
   python benchmark_complet_opsemia.py
   ```

7. **Consulter les r√©sultats**
   ```bash
   cat ../Docs\ Projet/BENCHMARK_RESULTATS_*.md
   ```

### D√©veloppement/D√©bogage

1. **Tester un mod√®le rapidement**
   ```bash
   python test_rapide_modele.py jina
   ```

2. **Modifier la configuration**
   - √âditer `benchmark_complet_opsemia.py`
   - Modifier `MODELES_TEXTE` (ajouter/retirer mod√®les)
   - Modifier chemins de fichiers

3. **Ajouter des requ√™tes**
   - √âditer `donnees_benchmark_opsemia.py`
   - Ajouter √† `REQUETES_MESSAGES` ou `REQUETES_IMAGES`

4. **Relancer la validation**
   ```bash
   python valider_benchmark.py
   ```

---

## üìä Fichiers de Donn√©es

### Dataset Source
- **Cas/Cas4/sms.csv** - 560 messages Breaking Bad
- **Cas/Cas4/images.csv** - M√©tadonn√©es de 28 images
- **Cas/Cas4/Images/** - Dossier des images

### Dataset Utilis√©
- **Messages:** 560 messages complets
- **Images:** 10 images s√©lectionn√©es
- **Requ√™tes:** 100 (90 messages + 10 images)

---

## üéØ R√©sultats Attendus

### Dur√©es d'Encodage Typiques

| Mod√®le | 1000 Messages | Est. 1M Messages |
|--------|---------------|------------------|
| Jina-v3 | ~12s | ~3.5h |
| Solon-large | ~15s | ~4.2h |
| BGE-M3 | ~18s | ~5h |
| Qwen3-8B (API) | ~250s | ~69h |

| Images | 10 Images | 1000 Images |
|--------|-----------|-------------|
| BLIP | ~20s | ~33h |

### M√©triques de Qualit√© Typiques

| Mod√®le | P@5 | R@5 | MRR |
|--------|-----|-----|-----|
| Meilleur | 0.85-0.90 | 0.80-0.85 | 0.75-0.80 |
| Moyen | 0.75-0.85 | 0.70-0.80 | 0.65-0.75 |

---

## üÜò Aide et Support

### En cas de probl√®me

1. **V√©rifier les pr√©requis**
   ```bash
   python valider_benchmark.py
   ```

2. **Consulter la documentation**
   - [GUIDE_DEMARRAGE_BENCHMARK.md](GUIDE_DEMARRAGE_BENCHMARK.md) - Guide rapide
   - [README_BENCHMARK.md](README_BENCHMARK.md) - Documentation compl√®te

3. **Tester un mod√®le isol√©ment**
   ```bash
   python test_rapide_modele.py [modele]
   ```

4. **V√©rifier les logs**
   - Les erreurs s'affichent dans la console
   - Rechercher les lignes commen√ßant par `‚ùå`

### Probl√®mes Courants

| Probl√®me | Solution |
|----------|----------|
| DEEPINFRA_TOKEN non configur√© | `export DEEPINFRA_TOKEN="cl√©"` |
| Out of memory | R√©duire batch size ou utiliser CPU |
| Mod√®le ne charge pas | T√©l√©charger avec script d√©di√© |
| Requ√™tes images ignor√©es | Compl√©ter dans donnees_benchmark_opsemia.py |

---

## üìà √âvolution Future

### Fonctionnalit√©s Pr√©vues
- [ ] Support de mod√®les suppl√©mentaires
- [ ] Export des r√©sultats en JSON/CSV
- [ ] Graphiques de comparaison
- [ ] Benchmark incr√©mental (sauvegarder r√©sultats)

### Contributions
Pour ajouter un mod√®le ou am√©liorer le syst√®me:
1. Modifier `benchmark_complet_opsemia.py`
2. Ajouter aux `MODELES_TEXTE`
3. Cr√©er script de t√©l√©chargement si n√©cessaire
4. Tester avec `test_rapide_modele.py`

---

## üìù Notes de Version

**Version 1.0** (Octobre 2024)
- ‚úÖ Benchmark complet 4 mod√®les
- ‚úÖ Dataset 100 requ√™tes
- ‚úÖ Calcul dur√©es + qualit√©
- ‚úÖ G√©n√©ration rapport markdown
- ‚úÖ Support images via BLIP
- ‚úÖ API DeepInfra (Qwen3-8B)

---

*Index cr√©√© le 16/10/2024 - Syst√®me de Benchmark OPSEMIA*

